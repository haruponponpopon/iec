{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "570bb702",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kobayashi/anaconda3/envs/pytorch/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "from tqdm.auto import tqdm\n",
    "from torch import autocast\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import bisect\n",
    "import operator\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import sys\n",
    "import os\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46f2887d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'text_projection.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'visual_projection.weight', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'logit_scale', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.23.mlp.fc1.bias']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, PNDMScheduler\n",
    "\n",
    "# 1. Load the autoencoder model which will be used to decode the latents into image space. \n",
    "vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\")\n",
    "\n",
    "# 2. Load the tokenizer and text encoder to tokenize and encode the text. \n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "# 3. The UNet model for generating the latents.\n",
    "unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afacd28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kobayashi/anaconda3/envs/pytorch/lib/python3.9/site-packages/diffusers/schedulers/scheduling_dpmsolver_multistep.py:146: FutureWarning: The `predict_epsilon` argument is deprecated and will be removed in version 0.11.0. Please make sure to instantiate your scheduler with `prediction_type` instead. E.g. `scheduler = DPMSolverMultistepScheduler.from_pretrained(<model_id>, prediction_type='epsilon')`.\n",
      "  predict_epsilon = deprecate(\"predict_epsilon\", \"0.11.0\", message, take_from=kwargs)\n"
     ]
    }
   ],
   "source": [
    "# from diffusers import LMSDiscreteScheduler\n",
    "\n",
    "# scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
    "\n",
    "from diffusers import DPMSolverMultistepScheduler\n",
    "\n",
    "scheduler = DPMSolverMultistepScheduler(\n",
    "    beta_start=0.00085,\n",
    "    beta_end=0.012,\n",
    "    beta_schedule=\"scaled_linear\",\n",
    "    num_train_timesteps=1000,\n",
    "    trained_betas=None,\n",
    "    predict_epsilon=True,\n",
    "    thresholding=False,\n",
    "    algorithm_type=\"dpmsolver++\",\n",
    "    solver_type=\"midpoint\",\n",
    "    lower_order_final=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e6f7380",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = vae.to(torch_device)\n",
    "text_encoder = text_encoder.to(torch_device)\n",
    "unet = unet.to(torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8971dadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(latents,text_embeddings):\n",
    "    \n",
    "    num_inference_steps = 50            # Number of denoising steps\n",
    "\n",
    "    guidance_scale = 7.5                # Scale for classifier-free guidance\n",
    "\n",
    "    #############################################\n",
    "\n",
    "    max_length = text_input.input_ids.shape[-1]\n",
    "    uncond_input = tokenizer(\n",
    "        [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "      uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0]\n",
    "\n",
    "    ###############################################\n",
    "\n",
    "    embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
    "\n",
    "    #############################################\n",
    "\n",
    "    scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "    ##########################################\n",
    "\n",
    "    latents_1 = latents * scheduler.init_noise_sigma\n",
    "\n",
    "    #############################################\n",
    "\n",
    "\n",
    "\n",
    "    for t in tqdm(scheduler.timesteps):\n",
    "      # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
    "      latent_model_input_a = torch.cat([latents_1] * 2)\n",
    "\n",
    "      latent_model_input = scheduler.scale_model_input(latent_model_input_a, t)\n",
    "\n",
    "      # predict the noise residual\n",
    "      with torch.no_grad():\n",
    "        noise_pred = unet(latent_model_input, t, encoder_hidden_states=embeddings).sample\n",
    "\n",
    "      # perform guidance\n",
    "      noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "      noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "      # compute the previous noisy sample x_t -> x_t-1\n",
    "      latents_1 = scheduler.step(noise_pred, t, latents_1).prev_sample\n",
    "\n",
    "    ###############################################\n",
    "\n",
    "    # scale and decode the image latents with vae\n",
    "    latents_3 = 1 / 0.18215 * latents_1\n",
    "\n",
    "    with torch.no_grad():\n",
    "      image = vae.decode(latents_3).sample\n",
    "\n",
    "\n",
    "    ##########################################\n",
    "\n",
    "    image_1 = (image / 2 + 0.5).clamp(0, 1)\n",
    "    image_2 = image_1.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "    images = (image_2 * 255).round().astype(\"uint8\")\n",
    "    pil_images = [Image.fromarray(image) for image in images]\n",
    "    result_image = pil_images[0]\n",
    "    \n",
    "    return result_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2a7adeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gaの関数\n",
    "population = 30 #個体数\n",
    "generations = 100 #世代数\n",
    "X = 5\n",
    "Y = 6\n",
    "tournament = 5\n",
    "\n",
    "# mutation = 10 #突然変異の個体数\n",
    "# population = 3 #個体数\n",
    "# generations = 2 #世代数\n",
    "# X = 1\n",
    "# Y = 3\n",
    "# # mutation = 1 #突然変異の個体数\n",
    "# tournament = 3\n",
    "\n",
    "if X*Y!=population:\n",
    "    print('Error: parameter error', file=sys.stderr)\n",
    "    sys.exit(1)\n",
    "    \n",
    "gene_length = 64\n",
    "elite = 3 #エリートの数\n",
    "initializa_txt_num = 100 #初期化個体においてtxtのベクトルをどれくらい元から変異させるか\n",
    "batch_size = 1\n",
    "height = 512                        # default height of Stable Diffusion\n",
    "width = 512                         # default width of Stable Diffusion\n",
    "\n",
    "if max(elite,tournament)>population:\n",
    "    print('Error: parameter error', file=sys.stderr)\n",
    "    sys.exit(1)\n",
    "    \n",
    "def pil2cv(image):\n",
    "    ''' PIL型 -> OpenCV型 '''\n",
    "    new_image = np.array(image, dtype=np.uint8)\n",
    "    if new_image.ndim == 2:  # モノクロ\n",
    "        pass\n",
    "    elif new_image.shape[2] == 3:  # カラー\n",
    "        new_image = cv2.cvtColor(new_image, cv2.COLOR_RGB2BGR)\n",
    "    elif new_image.shape[2] == 4:  # 透過\n",
    "        new_image = cv2.cvtColor(new_image, cv2.COLOR_RGBA2BGRA)\n",
    "    return new_image\n",
    "\n",
    "\n",
    "#相関係数 大きければ大きいほどよい\n",
    "def calc_fitness(target,images):\n",
    "    fitness = []\n",
    "    for image in images:\n",
    "        target_hist = cv2.calcHist([target], [0], None, [256], [0, 256])\n",
    "        comparing_hist = cv2.calcHist([pil2cv(image)], [0], None, [256], [0, 256])\n",
    "        ret = cv2.compareHist(target_hist, comparing_hist, 0)\n",
    "#         ans = 25*(np.exp(max(0,ret)))-24 #適合度の式\n",
    "        fitness.append(ret)\n",
    "    return fitness\n",
    "\n",
    "#seed値を複数個用意\n",
    "def initialize_gene(text_embeddings):\n",
    "    arr = []\n",
    "    for i in range(population):\n",
    "        embeddings = text_embeddings.clone()\n",
    "        seed_here = random.randrange(1000)\n",
    "        latents_torch = torch.randn(\n",
    "          (batch_size, unet.in_channels, height // 8, width // 8),\n",
    "          generator=torch.manual_seed(seed_here),\n",
    "        )\n",
    "        latents = latents_torch.to(torch_device)\n",
    "        for j in range(initializa_txt_num):\n",
    "            a = random.randrange(77)\n",
    "            b = random.randrange(768)\n",
    "            text_embeddings[0][a][b] = np.random.randn()\n",
    "        arr.append([latents,embeddings])    \n",
    "    return arr\n",
    "\n",
    "\n",
    "def show_image(images,epoch):\n",
    "    os.mkdir('experiment_data/'+experiment_id+'/epoch'+str(epoch))\n",
    "    fig = plt.figure()\n",
    "    for i in range(len(images)):\n",
    "        arrPIL = np.asarray(images[i])\n",
    "        images[i].save('experiment_data/'+experiment_id+'/epoch'+str(epoch)+\"/\"+str(i)+\".png\")\n",
    "        ax1 = fig.add_subplot(X, Y, i+1)\n",
    "        ax1.set_title(str(i))\n",
    "        ax1.axis(\"off\")\n",
    "        plt.imshow(arrPIL)\n",
    "#     print(epoch)\n",
    "    plt.savefig('experiment_data/'+experiment_id+'/epoch'+str(epoch)+\"/\"+\"ttl.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def evolve(fitness,genes):\n",
    "    new_genes = []\n",
    "    \n",
    "    #エリート戦略\n",
    "    elite_array = []\n",
    "    for i in range(population):\n",
    "        extra_gene = copy.deepcopy(genes[i])\n",
    "        elite_array.append([fitness[i],extra_gene])\n",
    "    elite_array = sorted(elite_array, key=operator.itemgetter(0))\n",
    "    elite_array = list(reversed(elite_array))\n",
    "    for i in range(elite):\n",
    "        new_genes.append(elite_array[i][1])\n",
    "    \n",
    "    #mutation, cross overで使う配列の準備\n",
    "    index = []\n",
    "    num = 0\n",
    "    for i in range(population):\n",
    "        num += fitness[i]\n",
    "        index.append(num)\n",
    "    index.append(num)\n",
    "    \n",
    "    #mutation\n",
    "#     for i in range(mutation):\n",
    "#         p1 = random.randrange(num)\n",
    "#         g1 = bisect.bisect(index,p1)\n",
    "#         new_gene1 = copy.deepcopy(genes[g1])\n",
    "#         for j in range(4):\n",
    "#             for k in range(64):\n",
    "#                 for l in range(64):\n",
    "#                     mutation_flag = random.randrange(100)\n",
    "#                     if mutation_flag==0:\n",
    "#                         new_gene1[0][0][j][k][l] = np.random.randn()\n",
    "#         for j in range(77):\n",
    "#             for k in range(768):\n",
    "#                 mutation_flag = random.randrange(100)\n",
    "#                 if mutation_flag==0:\n",
    "#                     new_gene1[1][0][j][k] = np.random.randn()\n",
    "#         new_genes.append(new_gene1)\n",
    "\n",
    "    # cross over\n",
    "    for i in range(population-elite):\n",
    "        used1 = [0]*population \n",
    "        p1 = []\n",
    "        while len(p1)<tournament:\n",
    "            r1 = random.randrange(population)\n",
    "            if used1[r1]==0:\n",
    "                p1.append([fitness[r1],r1])\n",
    "        p1 = sorted(p1, key=operator.itemgetter(0))\n",
    "#         print(p1)\n",
    "        new_gene = copy.deepcopy(genes[p1[tournament-1][1]])\n",
    "#         print(p1[tournament-1][1])\n",
    "        used1 = [0]*population \n",
    "        p1 = []\n",
    "        while len(p1)<tournament:\n",
    "            r1 = random.randrange(population)\n",
    "            if used1[r1]==0:\n",
    "                p1.append([fitness[r1],r1])\n",
    "        p1 = sorted(p1, key=operator.itemgetter(0))\n",
    "#         print(p1)\n",
    "        new_gene1 = copy.deepcopy(genes[p1[tournament-1][1]])\n",
    "#         print(p1[tournament-1][1])\n",
    "        cross = random.randrange(gene_length)\n",
    "        for j in range(4):\n",
    "            for k in range(64):\n",
    "                for l in range(64):\n",
    "                    a = random.randrange(2)\n",
    "                    if a==0:\n",
    "                        new_gene[0][0][j][k][l] = new_gene1[0][0][j][k][l]\n",
    "        for j in range(77):\n",
    "            for k in range(768):\n",
    "                a = random.randrange(2)\n",
    "                if a==0:\n",
    "                    new_gene[1][0][j][k] = new_gene1[1][0][j][k]\n",
    "                    \n",
    "        for j in range(4):\n",
    "            for k in range(64):\n",
    "                for l in range(64):\n",
    "                    mutation_flag = random.randrange(300)\n",
    "                    if mutation_flag==0:\n",
    "                        new_gene[0][0][j][k][l] += np.random.randn()\n",
    "                        if new_gene[0][0][j][k][l]>= 5:\n",
    "                            new_gene[0][0][j][k][l] = 4.99\n",
    "                        if new_gene[0][0][j][k][l]<= -5:\n",
    "                            new_gene[0][0][j][k][l] = -4.99\n",
    "        for j in range(77):\n",
    "            for k in range(768):\n",
    "                mutation_flag = random.randrange(6000)\n",
    "                if mutation_flag==0:\n",
    "                    new_gene[1][0][j][k] += np.random.randn()\n",
    "                    if new_gene[1][0][j][k]>= 5:\n",
    "                        new_gene[1][0][j][k] = 4.99\n",
    "                    if new_gene[1][0][j][k]<= -5:\n",
    "                        new_gene[1][0][j][k] = -4.99\n",
    "\n",
    "        new_genes.append(new_gene)\n",
    "                    \n",
    "    if len(new_genes)!=population:\n",
    "        print('Error: evolve error', file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "    return new_genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "12bc00ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_experiment():\n",
    "    experiment_time_delta = datetime.timedelta(hours=9)\n",
    "    experiment_JST = datetime.timezone(experiment_time_delta, 'JST')\n",
    "    global experiment_id\n",
    "    experiment_now = datetime.datetime.now(experiment_JST)\n",
    "    experiment_id = experiment_now.strftime('%Y%m%d%H%M%S')\n",
    "    #ディレクトリ初期化\n",
    "    os.mkdir('experiment_data/'+experiment_id)\n",
    "    \n",
    "    prompt = [\"a beautiful photograph of mountains, green covered hills, meadows, sunset, sky\"]\n",
    "    # prompt = [\"a photograph of a rice field in summer with clear sky\"]\n",
    "    target_image = Image.open(\"target_image/mountain.png\").convert('RGB').resize((512,512))\n",
    "    target = pil2cv(target_image)\n",
    "    text_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "      text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\n",
    "\n",
    "    genes = initialize_gene(text_embeddings)\n",
    "\n",
    "    with autocast(\"cuda\"):\n",
    "        #初期配置\n",
    "        images = []\n",
    "        for i in range(population):\n",
    "            image = func(genes[i][0],genes[i][1])\n",
    "            images.append(image)\n",
    "        show_image(images,0)\n",
    "        #進化+選択\n",
    "        for i in range(1,generations+1):\n",
    "            #選択\n",
    "            fitness = calc_fitness(target,images)\n",
    "            with open('experiment_data/'+experiment_id+'/fitness.txt', 'a') as f:\n",
    "                print(fitness, file=f)\n",
    "            #進化\n",
    "            genes = evolve(fitness,genes)\n",
    "            #画像生成\n",
    "            images.clear()\n",
    "            for j in range(population):\n",
    "                image = func(genes[j][0],genes[j][1])\n",
    "                images.append(image)\n",
    "            #表示\n",
    "            show_image(images,i)\n",
    "        fitness = calc_fitness(target,images)\n",
    "        with open('experiment_data/'+experiment_id+'fitness.txt', 'a') as f:\n",
    "            print(fitness, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2ab1e788",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7b689f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cd2609c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "552fa916",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "82fcf0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccadd9fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
