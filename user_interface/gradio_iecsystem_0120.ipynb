{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab1c739d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kobayashi/anaconda3/envs/pytorch/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "from tqdm.auto import tqdm\n",
    "from torch import autocast\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import bisect\n",
    "import operator\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "import datetime\n",
    "import time\n",
    "np.set_printoptions(threshold=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2436859",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'text_projection.weight', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.14.layer_norm2.weight', 'visual_projection.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.9.layer_norm2.bias', 'logit_scale', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, PNDMScheduler\n",
    "\n",
    "# 1. Load the autoencoder model which will be used to decode the latents into image space. \n",
    "vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\")\n",
    "\n",
    "# 2. Load the tokenizer and text encoder to tokenize and encode the text. \n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "# 3. The UNet model for generating the latents.\n",
    "unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c119353",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import LMSDiscreteScheduler\n",
    "\n",
    "scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50f39015",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vae = vae.to(torch_device)\n",
    "text_encoder = text_encoder.to(torch_device)\n",
    "unet = unet.to(torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9afe45dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(latents,text_embeddings):\n",
    "    \n",
    "    num_inference_steps = 50            # Number of denoising steps\n",
    "\n",
    "    guidance_scale = 7.5                # Scale for classifier-free guidance\n",
    "\n",
    "    #############################################\n",
    "\n",
    "    uncond_input = tokenizer(\n",
    "        [\"\"] * batch_size, padding=\"max_length\", return_tensors=\"pt\"\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "      uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0]\n",
    "\n",
    "    ###############################################\n",
    "\n",
    "    embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
    "\n",
    "    #############################################\n",
    "\n",
    "    scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "    ##########################################\n",
    "\n",
    "    latents_1 = latents * scheduler.init_noise_sigma\n",
    "\n",
    "    #############################################\n",
    "\n",
    "\n",
    "\n",
    "    for t in tqdm(scheduler.timesteps):\n",
    "      # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
    "      latent_model_input_a = torch.cat([latents_1] * 2)\n",
    "\n",
    "      latent_model_input = scheduler.scale_model_input(latent_model_input_a, t)\n",
    "\n",
    "      # predict the noise residual\n",
    "      with torch.no_grad():\n",
    "        noise_pred = unet(latent_model_input, t, encoder_hidden_states=embeddings).sample\n",
    "\n",
    "      # perform guidance\n",
    "      noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "      noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "      # compute the previous noisy sample x_t -> x_t-1\n",
    "      latents_1 = scheduler.step(noise_pred, t, latents_1).prev_sample\n",
    "\n",
    "    ###############################################\n",
    "    # scale and decode the image latents with vae\n",
    "    latents_3 = 1 / 0.18215 * latents_1\n",
    "\n",
    "    with torch.no_grad():\n",
    "      image = vae.decode(latents_3).sample\n",
    "\n",
    "\n",
    "    ##########################################\n",
    "\n",
    "    image_1 = (image / 2 + 0.5).clamp(0, 1)\n",
    "    image_2 = image_1.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "    images = (image_2 * 255).round().astype(\"uint8\")\n",
    "    pil_images = [Image.fromarray(image) for image in images]\n",
    "    result_image = pil_images[0]\n",
    "    \n",
    "    return result_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3dc3ba4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gaの関数\n",
    "population = 9 #個体数\n",
    "show_image_x = 3\n",
    "show_image_y = 3\n",
    "\n",
    "\n",
    "if show_image_x*show_image_y!=population or population>9:\n",
    "    print('Error: parameter error', file=sys.stderr)\n",
    "    sys.exit(1)\n",
    "\n",
    "initializa_txt_num = 70 #初期化個体においてtxtのベクトルをどれくらい元から変異させるか\n",
    "batch_size = 1\n",
    "height = 512                        # default height of Stable Diffusion\n",
    "width = 512                        # default width of Stable Diffusion\n",
    "image_mutation_rate = 300\n",
    "text_mutation_rate = 6000\n",
    "\n",
    "\n",
    "#seed値を複数個用意\n",
    "def initialize_gene():\n",
    "    arr = []\n",
    "    for i in range(population):\n",
    "        embeddings = iec_text_embeddings.clone()\n",
    "        seed_here = random.randrange(10000)\n",
    "        latents_torch = torch.randn(\n",
    "          (batch_size, unet.in_channels, height // 8, width // 8),\n",
    "          generator=torch.manual_seed(seed_here),\n",
    "        )\n",
    "        latents = latents_torch.to(torch_device)\n",
    "        for j in range(initializa_txt_num):\n",
    "            a = random.randrange(77)\n",
    "            b = random.randrange(768)\n",
    "            embeddings[0][a][b] = np.random.randn()\n",
    "        arr.append([latents,embeddings])\n",
    "    return arr\n",
    "\n",
    "\n",
    "def save_image(diffusion_images,epoch,folder):\n",
    "    os.mkdir(folder+'/epoch'+str(epoch))\n",
    "    for i in range (len(diffusion_images)):\n",
    "        diffusion_images[i].save(folder+\"/epoch\"+str(epoch)+\"/\"+str(i)+\".png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "23f435e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOREIGN = 2\n",
    "mutation_rate = 1.0\n",
    "def evolve(selected,genes):\n",
    "    # ログ\n",
    "    with open('./user_experiment_data/'+experiment_id+'/evolve.txt', 'a') as f:\n",
    "        print(\"epoch\", iec_live_epoch-1, file=f)\n",
    "    evolved_genes,selected_genes = [],[]\n",
    "    for i in range(population):\n",
    "        if selected[i]:\n",
    "            selected_genes.append(genes[i])\n",
    "    delta = population - len(selected_genes)\n",
    "    x = max(0,delta-FOREIGN)\n",
    "    if selected_genes:\n",
    "        for i in range(x):\n",
    "            # ログ\n",
    "            with open('./user_experiment_data/'+experiment_id+'/evolve.txt', 'a') as f:\n",
    "                print(\"population\", i, file=f)\n",
    "            evolved_genes.extend([mutate(simple_crossover(selected_genes,selected),mutation_rate)])\n",
    "    else:\n",
    "        return initialize_gene()\n",
    "    x = min(FOREIGN,delta)\n",
    "    for i in range(population):\n",
    "        if selected[i]:\n",
    "            # ログ\n",
    "            with open('./user_experiment_data/'+experiment_id+'/evolve.txt', 'a') as f:\n",
    "                print(\"population\", i,\"mutation\", file=f)\n",
    "    evolved_genes.extend([mutate(selected_genes[i],mutation_rate) for i in range(len(selected_genes))])\n",
    "    for i in range(x):\n",
    "        # ログ\n",
    "        with open('./user_experiment_data/'+experiment_id+'/evolve.txt', 'a') as f:\n",
    "            print(\"population\", i,\"initialize\", file=f)\n",
    "        embeddings = iec_text_embeddings.clone()\n",
    "        seed_here = random.randrange(10000)\n",
    "        latents_torch = torch.randn(\n",
    "          (batch_size, unet.in_channels, height // 8, width // 8),\n",
    "          generator=torch.manual_seed(seed_here),\n",
    "        )\n",
    "        latents = latents_torch.to(torch_device)\n",
    "        for j in range(initializa_txt_num):\n",
    "            a = random.randrange(77)\n",
    "            b = random.randrange(768)\n",
    "            embeddings[0][a][b] = np.random.randn()\n",
    "        evolved_genes.append([latents,embeddings])  \n",
    "\n",
    "                    \n",
    "    if len(evolved_genes)!=population:\n",
    "        print('Error: evolve error', file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "    return evolved_genes\n",
    "\n",
    "def simple_crossover(gene_population,selected):\n",
    "    p1 = random.randint(0,len(gene_population)-1)\n",
    "    p2 = random.randint(0,len(gene_population)-1)\n",
    "    gene_cnt = 0\n",
    "    for i in range(population):\n",
    "        if selected[i]:\n",
    "            gene_cnt += 1\n",
    "            if p1==gene_cnt-1:\n",
    "                # ログ\n",
    "                with open('./user_experiment_data/'+experiment_id+'/evolve.txt', 'a') as f:\n",
    "                    print(\"crossover1\", i, file=f)\n",
    "            if p2==gene_cnt-1:\n",
    "                # ログ\n",
    "                with open('./user_experiment_data/'+experiment_id+'/evolve.txt', 'a') as f:\n",
    "                    print(\"crossover2\", i, file=f)\n",
    "    gene1 = gene_population[p1]\n",
    "    gene2 = gene_population[p2]\n",
    "    return crossover(gene1,gene2)\n",
    "\n",
    "def crossover(gene1,gene2):\n",
    "    new_gene = copy.deepcopy(gene1)\n",
    "    gene3 = copy.deepcopy(gene2)\n",
    "    for j in range(4):\n",
    "        for k in range(64):\n",
    "            for l in range(64):\n",
    "                a = random.randrange(2)\n",
    "                if a==0:\n",
    "                    new_gene[0][0][j][k][l] = gene3[0][0][j][k][l]\n",
    "    for j in range(77):\n",
    "        for k in range(768):\n",
    "            new_gene[1][0][j][k] = gene3[1][0][j][k]\n",
    "    return new_gene\n",
    "\n",
    "def mutate(individual,mutation_rate = 1.0):\n",
    "    new_gene = copy.deepcopy(individual)\n",
    "    for j in range(4):\n",
    "        for k in range(64):\n",
    "            for l in range(64):\n",
    "                a = random.randrange(image_mutation_rate)\n",
    "                if a==0:\n",
    "                    value = individual[0][0][j][k][l]+np.random.randn()\n",
    "                    if value >= 5.00:\n",
    "                        value = 4.99\n",
    "                    elif value <=-5.00:\n",
    "                        value = -4.99\n",
    "                    new_gene[0][0][j][k][l] = value\n",
    "    for j in range(77):\n",
    "        for k in range(768):\n",
    "            a = random.randrange(text_mutation_rate)\n",
    "            if a==0:\n",
    "                value = individual[1][0][j][k]+np.random.randn()\n",
    "                if value >= 5.00:\n",
    "                    value = 4.99\n",
    "                elif value <=-5.00:\n",
    "                    value = -4.99\n",
    "                new_gene[1][0][j][k] = value\n",
    "    return new_gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "407b74ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7894\n",
      "Running on public URL: https://e1f0a3fe-0ed1-43a4.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://e1f0a3fe-0ed1-43a4.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:05<00:00,  8.41it/s]\n",
      "/tmp/ipykernel_55224/414557727.py:122: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  outputs.append(iec_image.resize((256,256),Image.ANTIALIAS))\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.45it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.49it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.50it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.51it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.71it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.10it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.10it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.08it/s]\n",
      "/tmp/ipykernel_55224/414557727.py:265: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  img = Image.open(iec_image_save_path+'/epoch'+str(iec_live_epoch)+'/' + str(i) + '.png').resize((256,256),Image.ANTIALIAS)\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.09it/s]\n",
      "/tmp/ipykernel_55224/414557727.py:51: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  outputs.append(iec_image.resize((256,256),Image.ANTIALIAS))\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.14it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.95it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.85it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.92it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.05it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.09it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.97it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.02it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.46it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.51it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.47it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.45it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.43it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.42it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.42it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.42it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.42it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.45it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.45it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.43it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.44it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.46it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.44it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.43it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.42it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.47it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.37it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.44it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.43it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.41it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.42it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.41it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.40it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.39it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.39it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.37it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.42it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.41it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.40it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.40it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.40it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.38it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.40it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.43it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.46it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.47it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.46it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.45it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.46it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.45it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.49it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.46it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.43it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.56it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.44it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.41it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.39it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.39it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.37it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.38it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.39it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.36it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.41it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.50it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.49it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.56it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.48it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.42it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.40it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.41it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.40it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.82it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.09it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.08it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.06it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.79it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.85it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.43it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.38it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.37it/s]\n",
      "/tmp/ipykernel_55224/414557727.py:26: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  img = Image.open(iec_image_save_path+'/epoch'+str(iec_live_epoch)+'/' + str(i) + '.png').resize((256,256),Image.ANTIALIAS)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kobayashi/anaconda3/envs/pytorch/lib/python3.9/site-packages/gradio/routes.py\", line 337, in run_predict\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/home/kobayashi/anaconda3/envs/pytorch/lib/python3.9/site-packages/gradio/blocks.py\", line 1015, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/home/kobayashi/anaconda3/envs/pytorch/lib/python3.9/site-packages/gradio/blocks.py\", line 833, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"/home/kobayashi/anaconda3/envs/pytorch/lib/python3.9/site-packages/anyio/to_thread.py\", line 31, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"/home/kobayashi/anaconda3/envs/pytorch/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 937, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/home/kobayashi/anaconda3/envs/pytorch/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 867, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/tmp/ipykernel_55224/414557727.py\", line 258, in iec_update_click\n",
      "    return generate_iec_images(*iec_selected_image)\n",
      "  File \"/tmp/ipykernel_55224/414557727.py\", line 29, in generate_iec_images\n",
      "    return iec_images\n",
      "UnboundLocalError: local variable 'iec_images' referenced before assignment\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kobayashi/anaconda3/envs/pytorch/lib/python3.9/site-packages/gradio/routes.py\", line 337, in run_predict\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/home/kobayashi/anaconda3/envs/pytorch/lib/python3.9/site-packages/gradio/blocks.py\", line 1015, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/home/kobayashi/anaconda3/envs/pytorch/lib/python3.9/site-packages/gradio/blocks.py\", line 833, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"/home/kobayashi/anaconda3/envs/pytorch/lib/python3.9/site-packages/anyio/to_thread.py\", line 31, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"/home/kobayashi/anaconda3/envs/pytorch/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 937, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/home/kobayashi/anaconda3/envs/pytorch/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 867, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/tmp/ipykernel_55224/414557727.py\", line 233, in show_result_image\n",
      "    conv_image = Image.open(conv_image_save_path+'/epoch'+\n",
      "  File \"/home/kobayashi/anaconda3/envs/pytorch/lib/python3.9/site-packages/PIL/Image.py\", line 3092, in open\n",
      "    fp = builtins.open(filename, \"rb\")\n",
      "FileNotFoundError: [Errno 2] No such file or directory: './user_experiment_data/20230126121045/conv/epoch0/0.png'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kobayashi/anaconda3/envs/pytorch/lib/python3.9/site-packages/gradio/routes.py\", line 337, in run_predict\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/home/kobayashi/anaconda3/envs/pytorch/lib/python3.9/site-packages/gradio/blocks.py\", line 1015, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/home/kobayashi/anaconda3/envs/pytorch/lib/python3.9/site-packages/gradio/blocks.py\", line 833, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"/home/kobayashi/anaconda3/envs/pytorch/lib/python3.9/site-packages/anyio/to_thread.py\", line 31, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"/home/kobayashi/anaconda3/envs/pytorch/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 937, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/home/kobayashi/anaconda3/envs/pytorch/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 867, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/tmp/ipykernel_55224/414557727.py\", line 258, in iec_update_click\n",
      "    return generate_iec_images(*iec_selected_image)\n",
      "  File \"/tmp/ipykernel_55224/414557727.py\", line 29, in generate_iec_images\n",
      "    return iec_images\n",
      "UnboundLocalError: local variable 'iec_images' referenced before assignment\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import PIL\n",
    "from PIL import Image,ImageTk\n",
    "iec_live_epoch=0\n",
    "conv_live_epoch=0\n",
    "iec_limit_epoch = 10\n",
    "conv_limit_epoch = 10\n",
    "practice_limit_epoch = 3\n",
    "conv_practice_epoch = 0\n",
    "iec_practice_epoch = 0\n",
    "iec_selected_image_id = 0\n",
    "conv_selected_image_id = 0\n",
    "iec_selected = []\n",
    "for i in range(population):\n",
    "    iec_selected.append(0)\n",
    "\n",
    "\n",
    "#本番、２回目以降のiecで画像を更新\n",
    "def generate_iec_images(*iec_selected_image):\n",
    "    global iec_live_epoch,iec_selected\n",
    "    outputs = []\n",
    "    #上限のときは、さっき表示されていた画像が表示される\n",
    "    if iec_live_epoch==iec_limit_epoch:\n",
    "        outputs.append(\"上限に到達しました。画像を一枚選んで終わるボタンを押してください\")\n",
    "        for i in range(population):\n",
    "            img = Image.open(iec_image_save_path+'/epoch'+str(iec_live_epoch)+'/' + str(i) + '.png').resize((256,256),Image.ANTIALIAS)\n",
    "            outputs.append(img)\n",
    "            outputs.append(False)\n",
    "        return outputs\n",
    "    for i in range(population):\n",
    "        if iec_selected_image[i]:\n",
    "            iec_selected[i]=1\n",
    "        else:\n",
    "            iec_selected[i]=0\n",
    "    iec_live_epoch += 1\n",
    "    # ログ\n",
    "    with open('./user_experiment_data/'+experiment_id+'/iec_select.txt', 'a') as f:\n",
    "        print(time.time(), \"epoch\"+str(iec_live_epoch-1),\n",
    "              iec_selected, file=f)\n",
    "    #進化\n",
    "    global genes\n",
    "    genes = evolve(iec_selected,genes)\n",
    "\n",
    "    #画像生成\n",
    "    iec_images = []\n",
    "    outputs.append(\"試行回数: \"+str(iec_live_epoch)+\"  残り回数: \"+str(iec_limit_epoch-iec_live_epoch))\n",
    "    with autocast(\"cuda\"):\n",
    "        for i in range(population):\n",
    "            iec_image = func(genes[i][0],genes[i][1])\n",
    "            iec_images.append(iec_image)\n",
    "            outputs.append(iec_image.resize((256,256),Image.ANTIALIAS))\n",
    "            outputs.append(False)\n",
    "    save_image(iec_images,iec_live_epoch,iec_image_save_path)\n",
    "    for i in range(population):\n",
    "        with open('./user_experiment_data/'+experiment_id+\"/iec/epoch\"+str(iec_live_epoch)+\"/\"+str(i)+\"text.txt\", 'a') as f:\n",
    "            x_tensor = genes[i][1]\n",
    "            device = x_tensor.device\n",
    "            x_numpy = x_tensor.to('cpu').detach().numpy().copy()\n",
    "            x_tensor = x_tensor.to(device)\n",
    "            print(x_numpy,file=f)\n",
    "    # ログ\n",
    "    with open('./user_experiment_data/'+experiment_id+'/iec_time_log.txt', 'a') as f:\n",
    "        print(time.time(), \"finished_generaging_image_epoch\"+str(iec_live_epoch),file=f)\n",
    "    return outputs\n",
    "#本番、２回目以降のconvで画像を更新\n",
    "def generate_conv_images(input_prompt):\n",
    "    global conv_live_epoch\n",
    "    #上限のときは、さっき表示されていた画像が表示される\n",
    "    if conv_live_epoch==conv_limit_epoch:\n",
    "        conv_images = []\n",
    "        conv_images.append(\"上限に到達しました。画像を一枚選んで終わるボタンを押してください\")\n",
    "        for i in range(population):\n",
    "            img = Image.open(conv_image_save_path+'/epoch'+str(conv_live_epoch)+'/' + str(i) + '.png').resize((256,256),Image.ANTIALIAS)\n",
    "            conv_images.append(img)\n",
    "        return conv_images\n",
    "    conv_live_epoch += 1\n",
    "    \n",
    "    # ログ\n",
    "    with open('./user_experiment_data/'+experiment_id+'/conv_prompt.txt', 'a') as f:\n",
    "        print(\"epoch\"+str(conv_live_epoch),\n",
    "              input_prompt, file=f)\n",
    "    text_input = tokenizer([input_prompt], padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        conv_text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\n",
    "    conv_images = []\n",
    "    conv_images.append(\"試行回数: \"+str(conv_live_epoch)+\"  残り回数: \"+str(conv_limit_epoch-conv_live_epoch))\n",
    "    for i in range(population):\n",
    "        seed_here = random.randrange(10000)\n",
    "        latents_torch = torch.randn(\n",
    "          (batch_size, unet.in_channels, height // 8, width // 8),\n",
    "          generator=torch.manual_seed(seed_here),\n",
    "        )\n",
    "        latents = latents_torch.to(torch_device)\n",
    "        with autocast(\"cuda\"):\n",
    "            conv_image = func(latents,conv_text_embeddings)\n",
    "        conv_images.append(conv_image)\n",
    "    save_image(conv_images[1:],conv_live_epoch,conv_image_save_path)\n",
    "    for i in range(1,population+1):\n",
    "        conv_images[i]=conv_images[i].resize((256,256),Image.ANTIALIAS)\n",
    "    # ログ\n",
    "    with open('./user_experiment_data/'+experiment_id+'/conv_time_log.txt', 'a') as f:\n",
    "        print(time.time(), \"finished_generaging_image_epoch\"+str(conv_live_epoch),file=f)\n",
    "    return conv_images\n",
    "# 本番、一回目のiec画像生成\n",
    "def generate_iec_initial_images(input_prompt):\n",
    "    iec_images = []\n",
    "    outputs = []\n",
    "    global iec_live_epoch\n",
    "    iec_live_epoch += 1\n",
    "    outputs.append(\"試行回数: \"+str(iec_live_epoch)+\"  残り回数: \"+str(iec_limit_epoch-iec_live_epoch))\n",
    "    text_input = tokenizer([input_prompt], padding=\"max_length\", \n",
    "    max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "    global iec_text_embeddings\n",
    "    with torch.no_grad():\n",
    "        iec_text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\n",
    "    global genes\n",
    "    genes = initialize_gene()\n",
    "    with autocast(\"cuda\"):\n",
    "        for i in range(population):\n",
    "            iec_image = func(genes[i][0],genes[i][1])\n",
    "            iec_images.append(iec_image)\n",
    "            outputs.append(iec_image.resize((256,256),Image.ANTIALIAS))\n",
    "            outputs.append(False)\n",
    "    save_image(iec_images,iec_live_epoch,iec_image_save_path)\n",
    "    for i in range(population):\n",
    "        with open('./user_experiment_data/'+experiment_id+\"/iec/epoch\"+str(iec_live_epoch)+\"/\"+str(i)+\"text.txt\", 'a') as f:\n",
    "            x_tensor = genes[i][1]\n",
    "            device = x_tensor.device\n",
    "            x_numpy = x_tensor.to('cpu').detach().numpy().copy()\n",
    "            x_tensor = x_tensor.to(device)\n",
    "            print(x_numpy,file=f)\n",
    "    # ログ\n",
    "    with open('./user_experiment_data/'+experiment_id+'/iec_time_log.txt', 'a') as f:\n",
    "        print(time.time(), \"finished_generaging_image_epoch\"+str(iec_live_epoch),file=f)\n",
    "    return outputs\n",
    "# 本番、一回目のconv画像生成\n",
    "def generate_conv_initial_images(input_prompt):\n",
    "    conv_images = []\n",
    "    global conv_live_epoch,conv_text_embeddings\n",
    "\n",
    "    conv_live_epoch += 1\n",
    "    conv_images.append(\"試行回数: \"+str(conv_live_epoch)+\"  残り回数: \"+str(conv_limit_epoch-conv_live_epoch))\n",
    "    # ログ\n",
    "    with open('./user_experiment_data/'+experiment_id+'/conv_prompt.txt', 'a') as f:\n",
    "        print(\"epoch\"+str(conv_live_epoch),\n",
    "              input_prompt, file=f)\n",
    "    text_input = tokenizer([input_prompt], padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        conv_text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\n",
    "    \n",
    "    for i in range(population):\n",
    "        seed_here = random.randrange(10000)\n",
    "        latents_torch = torch.randn(\n",
    "          (batch_size, unet.in_channels, height // 8, width // 8),\n",
    "          generator=torch.manual_seed(seed_here),\n",
    "        )\n",
    "        latents = latents_torch.to(torch_device)\n",
    "        with autocast(\"cuda\"):\n",
    "            conv_image = func(latents,conv_text_embeddings)\n",
    "        conv_images.append(conv_image)\n",
    "    save_image(conv_images[1:],conv_live_epoch,conv_image_save_path)\n",
    "    for i in range(1,population+1):\n",
    "        conv_images[i]=conv_images[i].resize((256,256),Image.ANTIALIAS)\n",
    "    # ログ\n",
    "    with open('./user_experiment_data/'+experiment_id+'/conv_time_log.txt', 'a') as f:\n",
    "        print(time.time(), \"finished_generaging_image_epoch\"+str(conv_live_epoch),file=f)\n",
    "    return conv_images\n",
    "#練習用の画像を返す\n",
    "def generate_iec_fake_images():\n",
    "    global iec_practice_epoch\n",
    "    #上限のときは、さっき表示されていた画像が表示される\n",
    "    if iec_practice_epoch==practice_limit_epoch:\n",
    "        iec_images = []\n",
    "        iec_images.append(\"上限に到達しました。画像を一枚選んで終わるボタンを押してください\")\n",
    "        for i in range(population):\n",
    "            img = Image.open(\"sample_image/iec/epoch\"+str(iec_practice_epoch)+\"/\"+str(i)+\".png\").resize((256,256),Image.ANTIALIAS)\n",
    "            iec_images.append(img)\n",
    "            iec_images.append(False)\n",
    "        return iec_images\n",
    "    iec_practice_epoch+=1\n",
    "    images = []\n",
    "    images.append(\"試行回数: \"+str(iec_practice_epoch)+\"  残り回数: \"\n",
    "                  +str(practice_limit_epoch-iec_practice_epoch))\n",
    "    for i in range(population):\n",
    "        image = Image.open(\"sample_image/iec/epoch\"+str(iec_practice_epoch)+\"/\"+str(i)+\".png\").resize((256,256),Image.ANTIALIAS)\n",
    "        images.append(image)\n",
    "        images.append(False)\n",
    "    return images\n",
    "#練習用の画像を返す\n",
    "def generate_conv_fake_images():\n",
    "    global conv_practice_epoch\n",
    "    #上限のときは、さっき表示されていた画像が表示される\n",
    "    if conv_practice_epoch==practice_limit_epoch:\n",
    "        conv_images = []\n",
    "        conv_images.append(\"上限に到達しました。画像を一枚選んで終わるボタンを押してください\")\n",
    "        for i in range(population):\n",
    "            img = Image.open(\"sample_image/iec/epoch\"+str(conv_practice_epoch)+\"/\"+str(i)+\".png\").resize((256,256),Image.ANTIALIAS)\n",
    "            conv_images.append(img)\n",
    "        return conv_images\n",
    "    conv_practice_epoch+=1\n",
    "    images = []\n",
    "    images.append(\"試行回数: \"+str(conv_practice_epoch)+\"  残り回数: \"\n",
    "                  +str(practice_limit_epoch-conv_practice_epoch))\n",
    "    for i in range(population):\n",
    "        image = Image.open(\"sample_image/iec/epoch\"+str(conv_practice_epoch)+\"/\"+str(i)+\".png\").resize((256,256),Image.ANTIALIAS)\n",
    "        images.append(image)\n",
    "    return images\n",
    "def decide_iec_image(index):\n",
    "    global iec_selected_image_id\n",
    "    iec_selected_image_id = index\n",
    "    # ログ\n",
    "    with open('./user_experiment_data/'+experiment_id+'/iec_time_log.txt', 'a') as f:\n",
    "        print(time.time(), \"finish_button_clicked_epoch\"+str(iec_live_epoch), file=f)\n",
    "    return \"お疲れ様です\"\n",
    "def decide_conv_image(index):\n",
    "    global conv_selected_image_id\n",
    "    conv_selected_image_id = index\n",
    "    # ログ\n",
    "    with open('./user_experiment_data/'+experiment_id+'/conv_time_log.txt', 'a') as f:\n",
    "        print(time.time(), \"finish_button_clicked_epoch\"+str(conv_live_epoch), file=f)\n",
    "    return \"お疲れ様です\"\n",
    "    \n",
    "#結果画像の表示\n",
    "def show_result_image():\n",
    "    # ログ\n",
    "    with open('./user_experiment_data/'+experiment_id+'/result.txt', 'a') as f:\n",
    "        print(\"iec epoch\",iec_live_epoch,\"image\", iec_selected_image_id,file=f)\n",
    "    # ログ\n",
    "    with open('./user_experiment_data/'+experiment_id+'/result.txt', 'a') as f:\n",
    "        print(\"conv epoch\",conv_live_epoch,\"image\", conv_selected_image_id,file=f)\n",
    "    iec_image = Image.open(iec_image_save_path\n",
    "                           +'/epoch'+str(iec_live_epoch)+'/' + str(iec_selected_image_id) + '.png')\n",
    "    conv_image = Image.open(conv_image_save_path+'/epoch'+\n",
    "                            str(conv_live_epoch)+'/' + str(conv_selected_image_id) + '.png')\n",
    "    return [iec_image,conv_image]\n",
    "def reading_text_file(file_name):\n",
    "    file = open(file_name, 'r')\n",
    "    text_data = file.read()\n",
    "    return text_data\n",
    "\n",
    "#convの画像生成\n",
    "def conv_update_click(text_prompt):\n",
    "    # ログ\n",
    "    with open('./user_experiment_data/'+experiment_id+'/conv_time_log.txt', 'a') as f:\n",
    "        print(time.time(), \"update_button_clicked_epoch\"+str(conv_live_epoch), file=f)\n",
    "    if conv_live_epoch==0:\n",
    "        return generate_conv_initial_images(text_prompt)\n",
    "    else:\n",
    "        return generate_conv_images(text_prompt)\n",
    "#iecの画像生成\n",
    "def iec_update_click(input_prompt,*iec_selected_image):\n",
    "    # ログ\n",
    "    with open('./user_experiment_data/'+experiment_id+'/iec_time_log.txt', 'a') as f:\n",
    "        print(time.time(), \"update_button_clicked_epoch\"+str(iec_live_epoch), file=f)\n",
    "    if iec_live_epoch==0:\n",
    "        return generate_iec_initial_images(input_prompt)\n",
    "    else:\n",
    "        return generate_iec_images(*iec_selected_image)\n",
    "def fake_end():\n",
    "    return \"お疲れ様でした\"\n",
    "def show_iec_current_images():\n",
    "    iec_images = []\n",
    "    if iec_live_epoch==0:\n",
    "        return generate_iec_fake_images()\n",
    "    iec_images.append(\"試行回数: \"+str(iec_live_epoch)+\"  残り回数: \"+str(iec_limit_epoch-iec_live_epoch))\n",
    "    for i in range(population):\n",
    "        img = Image.open(iec_image_save_path+'/epoch'+str(iec_live_epoch)+'/' + str(i) + '.png').resize((256,256),Image.ANTIALIAS)\n",
    "        iec_images.append(img)\n",
    "        iec_images.append(False)\n",
    "    return iec_images\n",
    "def show_conv_current_images():\n",
    "    conv_images = []\n",
    "    if conv_live_epoch==0:\n",
    "        return generate_conv_fake_images()\n",
    "    conv_images.append(\"試行回数: \"+str(conv_live_epoch)+\"  残り回数: \"+str(conv_limit_epoch-conv_live_epoch))\n",
    "    for i in range(population):\n",
    "        img = Image.open(conv_image_save_path+'/epoch'+str(conv_live_epoch)+'/' + str(i) + '.png').resize((256,256),Image.ANTIALIAS)\n",
    "        conv_images.append(img)\n",
    "    return conv_images\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Tab(\"練習 1\"):\n",
    "        output = []\n",
    "        population_check = []\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                gr.Markdown(\"### お題: \"+reading_text_file('src/practice_topic.txt'))\n",
    "                gr.Markdown(reading_text_file('src/iec_howto.txt'))\n",
    "                input_prompt = gr.Textbox(label=\"文章を入力してください\",\n",
    "                                          lines=5,value = reading_text_file('src/practice_input_text.txt'))\n",
    "                guidline = gr.Textbox(\"試行回数: \"+str(iec_practice_epoch)+\"  残り回数: \"\n",
    "                                         +str(practice_limit_epoch-iec_practice_epoch),show_label=False)\n",
    "                output.append(guidline)\n",
    "                continue_btn = gr.Button(\"画像を更新\")\n",
    "                iec_select_dropdown=gr.Dropdown(list(range(population)),label=\"気に入った画像\"),\n",
    "                end_btn = gr.Button(\"終わる\")\n",
    "            for i in range(show_image_x):    \n",
    "                with gr.Column():\n",
    "                    for j in range(show_image_y):\n",
    "                        with gr.Row():\n",
    "                            output.append(gr.Image(label=str(i*show_image_y+j)).style(height=200,width=200))\n",
    "                        pop_check = gr.Checkbox(label=\"👍\",interactive=True)\n",
    "                        population_check.append(pop_check)\n",
    "                        output.append(pop_check)\n",
    "        continue_btn.click(fn=generate_iec_fake_images, inputs=None, outputs=output)\n",
    "        end_btn.click(fn=fake_end,inputs=None,outputs=guidline)\n",
    "    with gr.Tab(\"練習 2\"):\n",
    "        output = []\n",
    "        population_check = []\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                gr.Markdown(\"### お題: \"+reading_text_file('src/practice_topic.txt'))\n",
    "                gr.Markdown(reading_text_file('src/conv_howto.txt'))\n",
    "                input_prompt = gr.Textbox(label=\"文章を入力してください\",\n",
    "                                        lines=5,value = reading_text_file('src/practice_input_text.txt'),\n",
    "                                         interactive=True)\n",
    "                guidline = gr.Textbox(\"試行回数: \"+str(conv_practice_epoch)+\"  残り回数: \"\n",
    "                                         +str(practice_limit_epoch-conv_practice_epoch),show_label=False)\n",
    "                output.append(guidline)\n",
    "                continue_btn = gr.Button(\"画像を更新\")\n",
    "                iec_select_dropdown=gr.Dropdown(list(range(population)),label=\"気に入った画像\"),\n",
    "                end_btn = gr.Button(\"終わる\")\n",
    "            for i in range(show_image_x):    \n",
    "                with gr.Column():\n",
    "                    for j in range(show_image_y):\n",
    "                        with gr.Row():\n",
    "                            output.append(gr.Image(label=str(i*show_image_y+j)).style(height=200,width=200))\n",
    "        continue_btn.click(fn=generate_conv_fake_images, inputs=None, outputs=output)\n",
    "        end_btn.click(fn=fake_end,inputs=None,outputs=guidline)\n",
    "    with gr.Tab(\"実験 1\"):\n",
    "        output = []\n",
    "        continue_input = []\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                gr.Markdown(\"### お題: \"+reading_text_file('src/topic1.txt'))\n",
    "                gr.Markdown(reading_text_file('src/iec_howto.txt'))\n",
    "                continue_input.append(gr.Textbox(label=\"文章を入力してください\",\n",
    "                lines=5,value = reading_text_file('src/prompt1.txt'),interactive=False))\n",
    "                guidline = gr.Textbox(\"試行回数: \"+str(iec_live_epoch)\n",
    "                                         +\"  残り回数: \"+str(iec_limit_epoch-iec_live_epoch),show_label=False)\n",
    "                output.append(guidline)\n",
    "                continue_btn = gr.Button(\"画像を更新\")\n",
    "                iec_select_dropdown=gr.Dropdown(list(range(population)),label=\"気に入った画像\")\n",
    "                end_btn = gr.Button(\"終わる\")\n",
    "                reset_btn = gr.Button(\"リセット\")\n",
    "            for i in range(show_image_x):    \n",
    "                with gr.Column():\n",
    "                    for j in range(show_image_y):\n",
    "                        with gr.Row():\n",
    "                            output.append(gr.Image(label=str(i*show_image_y+j)).style(height=200,width=200))\n",
    "                        pop_check = gr.Checkbox(label=\"👍\",interactive=True)\n",
    "                        continue_input.append(pop_check)\n",
    "                        output.append(pop_check)\n",
    "        continue_btn.click(fn=iec_update_click, inputs=continue_input, outputs=output)\n",
    "        end_btn.click(fn=decide_iec_image,inputs=iec_select_dropdown,outputs=guidline)\n",
    "        reset_btn.click(fn=show_iec_current_images,inputs=None,outputs=output)\n",
    "    with gr.Tab(\"実験 2\"):\n",
    "        output = []\n",
    "        population_check = []\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                gr.Markdown(\"### お題: \"+reading_text_file('src/topic2.txt'))\n",
    "                gr.Markdown(reading_text_file('src/conv_howto.txt'))\n",
    "                input_prompt = gr.Textbox(label=\"文章を入力してください\",\n",
    "                lines=5,value = reading_text_file('src/prompt2.txt'),interactive=True)\n",
    "                guidline = gr.Textbox(\"試行回数: \"+str(conv_live_epoch)+\"  残り回数: \"\n",
    "                                         +str(conv_limit_epoch-conv_live_epoch),show_label=False)\n",
    "                output.append(guidline)\n",
    "                continue_btn = gr.Button(\"画像を更新\")\n",
    "                conv_select_dropdown=gr.Dropdown(list(range(population)),label=\"気に入った画像\")\n",
    "                end_btn = gr.Button(\"終わる\")\n",
    "                reset_btn = gr.Button(\"リセット\")\n",
    "            for i in range(show_image_x):    \n",
    "                with gr.Column():\n",
    "                    for j in range(show_image_y):\n",
    "                        with gr.Row():\n",
    "                            output.append(gr.Image(label=str(i*show_image_y+j)).style(height=200,width=200))\n",
    "        continue_btn.click(fn=conv_update_click, inputs=input_prompt, outputs=output)\n",
    "        end_btn.click(fn=decide_conv_image,inputs=conv_select_dropdown,outputs=guidline)\n",
    "        reset_btn.click(fn=show_conv_current_images,inputs=None,outputs=output)\n",
    "    with gr.Tab(\"結果\"):\n",
    "        gr.Markdown(\"experiment_id: \"+str(experiment_id))\n",
    "        selected_images = []\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                gr.Markdown(\"### お題: \"+reading_text_file('src/topic1.txt'))\n",
    "                selected_images.append(gr.Image(label=\"実験1\").style(height=300,width=300))\n",
    "            with gr.Column():\n",
    "                gr.Markdown(\"### お題: \"+reading_text_file('src/topic2.txt'))\n",
    "                selected_images.append(gr.Image(label=\"実験2\").style(height=300,width=300))\n",
    "        show_btn = gr.Button(\"結果を表示する\")\n",
    "        show_btn.click(fn=show_result_image,inputs=None,outputs=selected_images)\n",
    "if __name__ == \"__main__\":\n",
    "    #被験者のidを振る\n",
    "    experiment_time_delta = datetime.timedelta(hours=9)\n",
    "    experiment_JST = datetime.timezone(experiment_time_delta, 'JST')\n",
    "    global experiment_id,iec_image_save_path,conv_image_save_path\n",
    "    experiment_now = datetime.datetime.now(experiment_JST)\n",
    "    experiment_id = experiment_now.strftime('%Y%m%d%H%M%S')\n",
    "    iec_image_save_path='./user_experiment_data/'+experiment_id+'/iec'\n",
    "    conv_image_save_path='./user_experiment_data/'+experiment_id+'/conv'\n",
    "    #ディレクトリ初期化\n",
    "    os.mkdir('./user_experiment_data/'+experiment_id)\n",
    "    os.mkdir(iec_image_save_path)\n",
    "    os.mkdir(conv_image_save_path)\n",
    "    demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7bc15b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7880\n",
      "Running on public URL: https://21ec9abf-8407-41ac.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://21ec9abf-8407-41ac.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:05<00:00,  8.63it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.63it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.65it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.61it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.61it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.59it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.59it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.59it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.56it/s]\n",
      "/tmp/ipykernel_55224/3693576669.py:169: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  conv_images[i]=conv_images[i].resize((256,256),Image.ANTIALIAS)\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.43it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.47it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.56it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.60it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.60it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.59it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.62it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.47it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.47it/s]\n",
      "/tmp/ipykernel_55224/3693576669.py:137: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  iec_images[i]=iec_images[i].resize((256,256),Image.ANTIALIAS)\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.55it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.59it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.57it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.55it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.51it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.52it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.51it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.56it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.53it/s]\n",
      "/tmp/ipykernel_55224/3693576669.py:101: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  conv_images[i]=conv_images[i].resize((256,256),Image.ANTIALIAS)\n"
     ]
    }
   ],
   "source": [
    "#debug用\n",
    "def show_iec_current_images():\n",
    "    iec_images = []\n",
    "    iec_images.append(\"試行回数: \"+str(iec_live_epoch)+\"  残り回数: \"+str(iec_limit_epoch-iec_live_epoch))\n",
    "    for i in range(population):\n",
    "        img = Image.open(iec_image_save_path+'/epoch'+str(iec_live_epoch)+'/' + str(i) + '.png').resize((256,256),Image.ANTIALIAS)\n",
    "        iec_images.append(img)\n",
    "    return iec_images\n",
    "def show_conv_current_images():\n",
    "    conv_images = []\n",
    "    conv_images.append(\"試行回数: \"+str(conv_live_epoch)+\"  残り回数: \"+str(conv_limit_epoch-conv_live_epoch))\n",
    "    for i in range(population):\n",
    "        img = Image.open(conv_image_save_path+'/epoch'+str(conv_live_epoch)+'/' + str(i) + '.png').resize((256,256),Image.ANTIALIAS)\n",
    "        conv_images.append(img)\n",
    "    return conv_images\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Tab(\"実験 1\"):\n",
    "        output = []\n",
    "        continue_input = []\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                gr.Markdown(reading_text_file('src/iec_howto.txt'))\n",
    "                continue_input.append(gr.Textbox(label=\"文章を入力してください\"))\n",
    "                guidline = gr.Textbox(\"試行回数: \"+str(iec_live_epoch)\n",
    "                                         +\"  残り回数: \"+str(iec_limit_epoch-iec_live_epoch),label=None)\n",
    "                output.append(guidline)\n",
    "                current_btn = gr.Button(\"現在の画像を表示\")\n",
    "                continue_btn = gr.Button(\"続ける\")\n",
    "                iec_select_dropdown=gr.Dropdown(list(range(population)),label=\"気に入った画像\")\n",
    "                end_btn = gr.Button(\"終わる\")\n",
    "            for i in range(show_image_x):    \n",
    "                with gr.Column():\n",
    "                    for j in range(show_image_y):\n",
    "                        with gr.Row():\n",
    "                            output.append(gr.Image(label=str(i*show_image_y+j)).style(height=200,width=200))\n",
    "                        continue_input.append(gr.Checkbox(label=\"👍\"))\n",
    "        current_btn.click(fn=show_iec_current_images,inputs=None,outputs=output)\n",
    "        continue_btn.click(fn=iec_update_click, inputs=continue_input, outputs=output)\n",
    "        end_btn.click(fn=decide_iec_image,inputs=iec_select_dropdown,outputs=guidline)\n",
    "    with gr.Tab(\"実験 2\"):\n",
    "        output = []\n",
    "        population_check = []\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                gr.Markdown(reading_text_file('src/conv_howto.txt'))\n",
    "                input_prompt = gr.Textbox(label=\"文章を入力してください\")\n",
    "                guidline = gr.Textbox(\"試行回数: \"+str(conv_live_epoch)+\"  残り回数: \"\n",
    "                                         +str(conv_limit_epoch-conv_live_epoch),label=None)\n",
    "                output.append(guidline)\n",
    "                current_btn = gr.Button(\"現在の画像を表示\")\n",
    "                continue_btn = gr.Button(\"続ける\")\n",
    "                conv_select_dropdown=gr.Dropdown(list(range(population)),label=\"気に入った画像\")\n",
    "                end_btn = gr.Button(\"終わる\")\n",
    "            for i in range(show_image_x):    \n",
    "                with gr.Column():\n",
    "                    for j in range(show_image_y):\n",
    "                        with gr.Row():\n",
    "                            output.append(gr.Image(label=str(i*show_image_y+j)).style(height=200,width=200))\n",
    "        current_btn.click(fn=show_conv_current_images,inputs=None,outputs=output)\n",
    "        continue_btn.click(fn=conv_update_click, inputs=input_prompt, outputs=output)\n",
    "        end_btn.click(fn=decide_conv_image,inputs=conv_select_dropdown,outputs=guidline)\n",
    "    with gr.Tab(\"結果\"):\n",
    "        show_btn = gr.Button(\"結果を表示する\")\n",
    "        with gr.Row():\n",
    "            selected_images = [gr.Image(label=\"実験1\").style(height=200,width=200),gr.Image(label=\"実験2\").style(height=200,width=200)]\n",
    "        show_btn.click(fn=show_result_image,inputs=None,outputs=selected_images)\n",
    "    demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aac8f82e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;cell line: 3&gt;</span>                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1 </span>image = Image.open(<span style=\"color: #808000; text-decoration-color: #808000\">\"sample_image/iec/epoch0/0.png\"</span>)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2 </span>r = [image,image]                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>3 p = [x.resize((<span style=\"color: #0000ff; text-decoration-color: #0000ff\">256</span>,<span style=\"color: #0000ff; text-decoration-color: #0000ff\">256</span>),Image.ANTIALIAS) <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> r]                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4 # r.resize((256,256),Image.ANTIALIAS)</span>                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">5 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008000; text-decoration-color: #008000\">'x'</span> is not defined\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<cell line: 3>\u001b[0m                                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1 \u001b[0mimage = Image.open(\u001b[33m\"\u001b[0m\u001b[33msample_image/iec/epoch0/0.png\u001b[0m\u001b[33m\"\u001b[0m)                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2 \u001b[0mr = [image,image]                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m3 p = [x.resize((\u001b[94m256\u001b[0m,\u001b[94m256\u001b[0m),Image.ANTIALIAS) \u001b[95min\u001b[0m r]                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m4 \u001b[0m\u001b[2m# r.resize((256,256),Image.ANTIALIAS)\u001b[0m                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m5 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mNameError: \u001b[0mname \u001b[32m'x'\u001b[0m is not defined\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = Image.open(\"sample_image/iec/epoch0/0.png\")\n",
    "r = [image,image]\n",
    "p = [x.resize((256,256),Image.ANTIALIAS) in r]\n",
    "# r.resize((256,256),Image.ANTIALIAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea8293a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
