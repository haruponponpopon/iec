{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2fa0f68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "from tqdm.auto import tqdm\n",
    "from torch import autocast\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import bisect\n",
    "import operator\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "import datetime\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "62fb7e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n",
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'text_projection.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'visual_projection.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'logit_scale', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, PNDMScheduler\n",
    "\n",
    "# 1. Load the autoencoder model which will be used to decode the latents into image space. \n",
    "vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\")\n",
    "\n",
    "# 2. Load the tokenizer and text encoder to tokenize and encode the text. \n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "# 3. The UNet model for generating the latents.\n",
    "unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "43bcf1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import LMSDiscreteScheduler\n",
    "\n",
    "scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "21bf5856",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = vae.to(torch_device)\n",
    "text_encoder = text_encoder.to(torch_device)\n",
    "unet = unet.to(torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "adaf8c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(latents,text_embeddings):\n",
    "    \n",
    "    num_inference_steps = 50            # Number of denoising steps\n",
    "\n",
    "    guidance_scale = 7.5                # Scale for classifier-free guidance\n",
    "\n",
    "    #############################################\n",
    "\n",
    "    uncond_input = tokenizer(\n",
    "        [\"\"] * batch_size, padding=\"max_length\", return_tensors=\"pt\"\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "      uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0]\n",
    "\n",
    "    ###############################################\n",
    "\n",
    "    embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
    "\n",
    "    #############################################\n",
    "\n",
    "    scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "    ##########################################\n",
    "\n",
    "    latents_1 = latents * scheduler.init_noise_sigma\n",
    "\n",
    "    #############################################\n",
    "\n",
    "\n",
    "\n",
    "    for t in tqdm(scheduler.timesteps):\n",
    "      # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
    "      latent_model_input_a = torch.cat([latents_1] * 2)\n",
    "\n",
    "      latent_model_input = scheduler.scale_model_input(latent_model_input_a, t)\n",
    "\n",
    "      # predict the noise residual\n",
    "      with torch.no_grad():\n",
    "        noise_pred = unet(latent_model_input, t, encoder_hidden_states=embeddings).sample\n",
    "\n",
    "      # perform guidance\n",
    "      noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "      noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "      # compute the previous noisy sample x_t -> x_t-1\n",
    "      latents_1 = scheduler.step(noise_pred, t, latents_1).prev_sample\n",
    "\n",
    "    ###############################################\n",
    "\n",
    "    # scale and decode the image latents with vae\n",
    "    latents_3 = 1 / 0.18215 * latents_1\n",
    "\n",
    "    with torch.no_grad():\n",
    "      image = vae.decode(latents_3).sample\n",
    "\n",
    "\n",
    "    ##########################################\n",
    "\n",
    "    image_1 = (image / 2 + 0.5).clamp(0, 1)\n",
    "    image_2 = image_1.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "    images = (image_2 * 255).round().astype(\"uint8\")\n",
    "    pil_images = [Image.fromarray(image) for image in images]\n",
    "    result_image = pil_images[0]\n",
    "    \n",
    "    return result_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "da6240b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gaの関数\n",
    "\n",
    "mutation = 0 #突然変異の個体数\n",
    "# population = 3 #個体数\n",
    "# X = 1\n",
    "# Y = 3\n",
    "mutation = 2 #突然変異の個体数\n",
    "\n",
    "if X*Y!=population or population>9:\n",
    "    print('Error: parameter error', file=sys.stderr)\n",
    "    sys.exit(1)\n",
    "    \n",
    "gene_length = 64\n",
    "elite = 0 #エリートの数\n",
    "initializa_txt_num = 70 #初期化個体においてtxtのベクトルをどれくらい元から変異させるか\n",
    "batch_size = 1\n",
    "height = 512                        # default height of Stable Diffusion\n",
    "width = 512                        # default width of Stable Diffusion\n",
    "image_mutation_rate = 400\n",
    "text_mutation_rate = 500\n",
    "\n",
    "if elite+mutation>population:\n",
    "    print('Error: parameter error', file=sys.stderr)\n",
    "    sys.exit(1)\n",
    "\n",
    "#seed値を複数個用意\n",
    "def initialize_gene(text_embeddings):\n",
    "    arr = []\n",
    "    for i in range(population):\n",
    "        embeddings = text_embeddings.clone()\n",
    "        seed_here = random.randrange(1000)\n",
    "        latents_torch = torch.randn(\n",
    "          (batch_size, unet.in_channels, height // 8, width // 8),\n",
    "          generator=torch.manual_seed(seed_here),\n",
    "        )\n",
    "        latents = latents_torch.to(torch_device)\n",
    "        for j in range(initializa_txt_num):\n",
    "            a = random.randrange(77)\n",
    "            b = random.randrange(768)\n",
    "            text_embeddings[0][a][b] = np.random.randn()\n",
    "        arr.append([latents,embeddings])    \n",
    "    return arr\n",
    "\n",
    "\n",
    "def save_image(diffusion_images,epoch,folder):\n",
    "    os.mkdir(folder+'/epoch'+str(epoch))\n",
    "    for i in range (len(diffusion_images)):\n",
    "        diffusion_images[i].save(folder+\"/epoch\"+str(epoch)+\"/\"+str(i)+\".png\")\n",
    "    \n",
    "    \n",
    "def evolve(selected,genes):\n",
    "    global evolve_explanation\n",
    "    evolve_explanation = \"\"\n",
    "    new_genes = []\n",
    "    \n",
    "    #エリート戦略 #選ばれた画像は次世代に残す(?)\n",
    "#     for i in range(population):\n",
    "#         if selected[i]==1:\n",
    "#             adding_gene = copy.deepcopy(genes[i])\n",
    "#             evolve_explanation += str(len(new_genes))+\". elite\"+str(i)+\"\\n\"\n",
    "#             new_genes.append(adding_gene)\n",
    "    \n",
    "    #mutation, cross overで使う配列の準備\n",
    "    index = []\n",
    "    num = 0\n",
    "    for i in range(population):\n",
    "        num += 1+selected[i]*adapt\n",
    "        index.append(num)\n",
    "    index.append(num)\n",
    "    \n",
    "    #mutation\n",
    "    for i in range(mutation):\n",
    "        p1 = random.randrange(num)\n",
    "        g1 = bisect.bisect(index,p1)\n",
    "        evolve_explanation += str(len(new_genes))+\". mutation \"+str(g1)+\"\\n\"\n",
    "        new_gene1 = copy.deepcopy(genes[g1])\n",
    "        # 画像の元\n",
    "        for j in range(4):\n",
    "            for k in range(64):\n",
    "                for l in range(64):\n",
    "                    mutation_flag = random.randrange(image_mutation_rate)\n",
    "                    if mutation_flag==0:\n",
    "                        new_gene1[0][0][j][k][l] = np.random.randn()\n",
    "        #textの元\n",
    "        for j in range(77):\n",
    "            for k in range(768):\n",
    "                mutation_flag = random.randrange(text_mutation_rate)\n",
    "                if mutation_flag==0:\n",
    "                    new_gene1[1][0][j][k] = np.random.randn()\n",
    "        new_genes.append(new_gene1)\n",
    "\n",
    "    # cross over\n",
    "    for i in range(population-len(new_genes)):\n",
    "        p1 = random.randrange(num)\n",
    "        p2 = random.randrange(num)\n",
    "        g1 = bisect.bisect(index,p1)\n",
    "        g2 = bisect.bisect(index,p2)\n",
    "        new_gene = copy.deepcopy(genes[g1])\n",
    "        new_gene1 = copy.deepcopy(genes[g2])\n",
    "        evolve_explanation += str(len(new_genes))+\". crossover \"+str(g1)+\" and \"+str(g2)+\"\\n\"\n",
    "        cross = random.randrange(gene_length)\n",
    "        for j in range(4):\n",
    "            for k in range(64):\n",
    "                for l in range(64):\n",
    "                    a = random.randrange(2)\n",
    "                    if a==0:\n",
    "                        new_gene[0][0][j][k][l] = new_gene1[0][0][j][k][l]\n",
    "        for j in range(77):\n",
    "            for k in range(768):\n",
    "                new_gene[1][0][j][k] = new_gene1[1][0][j][k]\n",
    "        new_genes.append(new_gene)\n",
    "                    \n",
    "    if len(new_genes)!=population:\n",
    "        print('Error: evolve error', file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "    return new_genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c3df57b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [67]\u001b[0m, in \u001b[0;36m<cell line: 105>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    124\u001b[0m         first_generate_btn\u001b[38;5;241m.\u001b[39mclick(fn\u001b[38;5;241m=\u001b[39mswitch_to_live1, inputs\u001b[38;5;241m=\u001b[39minput_prompt, outputs\u001b[38;5;241m=\u001b[39moutput)\n\u001b[1;32m    125\u001b[0m         continue_btn\u001b[38;5;241m.\u001b[39mclick(fn\u001b[38;5;241m=\u001b[39mgenerate_iec_images, inputs\u001b[38;5;241m=\u001b[39mpopulation_check, outputs\u001b[38;5;241m=\u001b[39moutput)\n\u001b[0;32m--> 126\u001b[0m         end_btn\u001b[38;5;241m.\u001b[39mclick(fn\u001b[38;5;241m=\u001b[39mcheckbox_change,inputs\u001b[38;5;241m=\u001b[39mpopulation_check,outputs\u001b[38;5;241m=\u001b[39minput_prompt)\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m#     with gr.Tab(\"Practice 2\"):\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m#         end_btn = gr.Button(\"終わる\")\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/gradio/blocks.py:1098\u001b[0m, in \u001b[0;36mBlocks.__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1096\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mchildren\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren)\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_config_file()\n\u001b[0;32m-> 1098\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapp \u001b[38;5;241m=\u001b[39m \u001b[43mroutes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mApp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_app\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogress_tracking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m   1100\u001b[0m     block_fn\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m special_args(block_fn\u001b[38;5;241m.\u001b[39mfn)[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1101\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m block_fn \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfns\n\u001b[1;32m   1102\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/gradio/routes.py:136\u001b[0m, in \u001b[0;36mApp.create_app\u001b[0;34m(blocks)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_app\u001b[39m(blocks: gradio\u001b[38;5;241m.\u001b[39mBlocks) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m App:\n\u001b[1;32m    135\u001b[0m     app \u001b[38;5;241m=\u001b[39m App(default_response_class\u001b[38;5;241m=\u001b[39mORJSONResponse)\n\u001b[0;32m--> 136\u001b[0m     \u001b[43mapp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfigure_app\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblocks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m     app\u001b[38;5;241m.\u001b[39madd_middleware(\n\u001b[1;32m    139\u001b[0m         CORSMiddleware,\n\u001b[1;32m    140\u001b[0m         allow_origins\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    141\u001b[0m         allow_methods\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    142\u001b[0m         allow_headers\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    143\u001b[0m     )\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;129m@app\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/user\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;129m@app\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/user/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_current_user\u001b[39m(request: fastapi\u001b[38;5;241m.\u001b[39mRequest) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mstr\u001b[39m]:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/gradio/routes.py:124\u001b[0m, in \u001b[0;36mApp.configure_app\u001b[0;34m(self, blocks)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_queue\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks\u001b[38;5;241m.\u001b[39m_queue\u001b[38;5;241m.\u001b[39mset_access_token(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqueue_token)\n\u001b[0;32m--> 124\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcwd \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetcwd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfavicon_path \u001b[38;5;241m=\u001b[39m blocks\u001b[38;5;241m.\u001b[39mfavicon_path\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokens \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import PIL\n",
    "from PIL import Image,ImageTk\n",
    "current_system_is_practice1 = False\n",
    "current_system_is_practice2 = False\n",
    "current_system_is_live1 = False\n",
    "current_system_is_live2 = False\n",
    "population = 2 #個体数\n",
    "show_image_x = 1\n",
    "show_image_y = 2\n",
    "iec_live_epoch=0\n",
    "iec_selected = []\n",
    "for i in range(X*Y):\n",
    "    iec_selected.append(0)\n",
    "def initialize_image(input_prompt):\n",
    "    images = []\n",
    "    for i in range(population):\n",
    "        image = Image.open(\"sample_image/epoch0/\"+str(i)+\".png\")\n",
    "        images.append(image)\n",
    "    return images\n",
    "#本番、２回目以降の進化で画像を更新\n",
    "def generate_iec_images(*iec_selected_image):\n",
    "    global iec_live_epoch,iec_selected\n",
    "    for i in range(population):\n",
    "        if iec_selected_image[i]=='👍':\n",
    "            iec_selected[i]=1\n",
    "        else:\n",
    "            iec_selected[i]=0\n",
    "    with open('./log.txt', 'a') as f:\n",
    "        print(iec_selected)\n",
    "    print(iec_selected)\n",
    "    print(iec_live_epoch)\n",
    "    iec_live_epoch += 1\n",
    "    #進化\n",
    "    global genes\n",
    "    genes = evolve(iec_selected,genes)\n",
    "    #画像生成\n",
    "    iec_images = []\n",
    "    with autocast(\"cuda\"):\n",
    "        for i in range(population):\n",
    "            iec_image = func(genes[i][0],genes[i][1])\n",
    "            iec_images.append(iec_image)\n",
    "    save_image(iec_images,iec_live_epoch,image_save_path)\n",
    "    return iec_images\n",
    "# 本番、一回目のiec画像生成\n",
    "def generate_iec_initial_images(input_prompt):\n",
    "    text_input = tokenizer([input_prompt], padding=\"max_length\", \n",
    "    max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "      text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\n",
    "    global genes\n",
    "    genes = initialize_gene(text_embeddings)\n",
    "    iec_images = []\n",
    "    with autocast(\"cuda\"):\n",
    "        for i in range(population):\n",
    "            iec_image = func(genes[i][0],genes[i][1])\n",
    "            iec_images.append(iec_image)\n",
    "    save_image(iec_images,iec_epoch,image_save_path)\n",
    "    return iec_images\n",
    "#練習用の画像を返す\n",
    "def generate_fake_images(*input_prompt):\n",
    "    global epoch\n",
    "    epoch+=1\n",
    "    images = []\n",
    "    for i in range(population):\n",
    "        image = Image.open(\"sample_image/epoch\"+str(epoch)+\"/\"+str(i)+\".png\")\n",
    "        images.append(image)\n",
    "    return images\n",
    "\n",
    "def checkbox_change(checkbox,checkbox1):\n",
    "    ans=\"nothing\"\n",
    "#     for c in checkbox:\n",
    "    if checkbox:\n",
    "        ans=checkbox\n",
    "    if checkbox1:\n",
    "        ans+=checkbox1\n",
    "    return ans\n",
    "#     with open('./log.txt', 'a') as f:\n",
    "#         print(id)\n",
    "\n",
    "# 本番に飛んだら本番のグローバル変数をTrueにする\n",
    "def switch_to_live1(input_prompt):\n",
    "    #被験者のidを振る\n",
    "    experiment_time_delta = datetime.timedelta(hours=9)\n",
    "    experiment_JST = datetime.timezone(experiment_time_delta, 'JST')\n",
    "    global experiment_id,image_save_path\n",
    "    experiment_now = datetime.datetime.now(experiment_JST)\n",
    "    experiment_id = experiment_now.strftime('%Y%m%d%H%M%S')+\"iec\"\n",
    "    image_save_path='./user_experiment_data/'+experiment_id\n",
    "    #ディレクトリ初期化\n",
    "    os.mkdir('./user_experiment_data/'+experiment_id)\n",
    "    global current_system_is_practice1,current_system_is_practice2,current_system_is_live1,current_system_is_live2\n",
    "    current_system_is_practice1 = False\n",
    "    current_system_is_practice2 = False\n",
    "    current_system_is_live1 =True\n",
    "    current_system_is_live2 =False\n",
    "    return generate_iec_initial_images(input_prompt)\n",
    "\n",
    "def test_func(input_prompt):\n",
    "    created_images = []\n",
    "    for i in range(population):\n",
    "        image = Image.open(\"sample_image/epoch0/\"+str(i)+\".png\")\n",
    "        created_images.append(image)\n",
    "    return  created_images\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Tab(\"本番 1\"):\n",
    "        output = []\n",
    "        population_check = []\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                gr.Markdown(\"ここに操作方法の説明が入ります\")\n",
    "                input_prompt = gr.Textbox(label=\"文章を入力してください\")\n",
    "                gr.Markdown(\"世代数1\")\n",
    "                first_generate_btn = gr.Button(\"スタート\")\n",
    "                continue_btn = gr.Button(\"続ける\")\n",
    "                gr.Dropdown(list(range(population))),\n",
    "                end_btn = gr.Button(\"終わる\")\n",
    "            for i in range(show_image_x):    \n",
    "                with gr.Column():\n",
    "                    for j in range(show_image_y):\n",
    "                        with gr.Row():\n",
    "                            output.append(gr.Image())\n",
    "                        population_check.append(gr.Radio([\"👍\",\"👎\"],value=\"👎\",show_label=False))\n",
    "        first_generate_btn.click(fn=switch_to_live1, inputs=input_prompt, outputs=output)\n",
    "        continue_btn.click(fn=generate_iec_images, inputs=population_check, outputs=output)\n",
    "        end_btn.click(fn=checkbox_change,inputs=population_check,outputs=input_prompt)\n",
    "#     with gr.Tab(\"Practice 2\"):\n",
    "#         end_btn = gr.Button(\"終わる\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dd0b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from PIL import Image\n",
    "#指定されたparameterの枚数のimageを縦と横に並べて返すコード\n",
    "population = 9\n",
    "show_image_x = 3\n",
    "show_image_y = 3\n",
    "epoch=0\n",
    "def initialize_image(input_prompt):\n",
    "    images = []\n",
    "    for i in range(population):\n",
    "        image = Image.open(\"sample_image/epoch0/\"+str(i)+\".png\")\n",
    "        images.append(image)\n",
    "    return images\n",
    "def generate_image(*input_prompt):\n",
    "    global epoch\n",
    "    epoch+=1\n",
    "    images = []\n",
    "    for i in range(population):\n",
    "        image = Image.open(\"sample_image/epoch\"+str(epoch)+\"/\"+str(i)+\".png\")\n",
    "        images.append(image)\n",
    "    return images\n",
    "def checkbox_change(checkbox,checkbox1):\n",
    "    ans=\"nothing\"\n",
    "#     for c in checkbox:\n",
    "    if checkbox:\n",
    "        ans=checkbox\n",
    "    if checkbox1:\n",
    "        ans+=checkbox1\n",
    "    return ans\n",
    "#     with open('./log.txt', 'a') as f:\n",
    "#         print(id)\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Tab(\"Practice 1\"):\n",
    "        output = []\n",
    "        population_check = []\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                gr.Markdown(\"この中から良いと思った画像を好きなだけ選択してください\")\n",
    "                input_prompt = gr.Textbox(label=\"文章を入力してください\")\n",
    "                gr.Markdown(\"世代数1\")\n",
    "                first_generate_btn = gr.Button(\"初期化\")\n",
    "                continue_btn = gr.Button(\"続ける\")\n",
    "                gr.Dropdown(list(range(population))),\n",
    "                end_btn = gr.Button(\"終わる\")\n",
    "            for i in range(show_image_x):    \n",
    "                with gr.Column():\n",
    "                    for j in range(show_image_y):\n",
    "                        with gr.Row():\n",
    "                            output.append(gr.Image())\n",
    "                        population_check.append(gr.Radio([\"👍\",\"👎\"],value=\"👎\",show_label=False))\n",
    "        first_generate_btn.click(fn=initialize_image, inputs=input_prompt, outputs=output)\n",
    "        continue_btn.click(fn=generate_image, inputs=population_check, outputs=output)\n",
    "        end_btn.click(fn=checkbox_change,inputs=population_check,outputs=input_prompt)\n",
    "    with gr.Tab(\"Practice 2\"):\n",
    "        end_btn = gr.Button(\"終わる\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cfe202",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f002984c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
