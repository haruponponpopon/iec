{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9e6056e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "from tqdm.auto import tqdm\n",
    "from torch import autocast\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import bisect\n",
    "import operator\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1210e43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n",
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'logit_scale', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'visual_projection.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'text_projection.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, PNDMScheduler\n",
    "\n",
    "# 1. Load the autoencoder model which will be used to decode the latents into image space. \n",
    "vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\")\n",
    "\n",
    "# 2. Load the tokenizer and text encoder to tokenize and encode the text. \n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "# 3. The UNet model for generating the latents.\n",
    "unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69c685cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import LMSDiscreteScheduler\n",
    "\n",
    "scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db38ad27",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 146.00 MiB (GPU 0; 10.75 GiB total capacity; 320.87 MiB already allocated; 156.94 MiB free; 356.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m vae \u001b[38;5;241m=\u001b[39m vae\u001b[38;5;241m.\u001b[39mto(torch_device)\n\u001b[0;32m----> 2\u001b[0m text_encoder \u001b[38;5;241m=\u001b[39m \u001b[43mtext_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch_device\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m unet \u001b[38;5;241m=\u001b[39m unet\u001b[38;5;241m.\u001b[39mto(torch_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:987\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    984\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m    985\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> 987\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:639\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 639\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    642\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    643\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    644\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    649\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    650\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:639\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 639\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    642\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    643\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    644\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    649\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    650\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:639\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 639\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    642\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    643\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    644\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    649\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    650\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:662\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    660\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    661\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 662\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    663\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:985\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m    983\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    984\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> 985\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 146.00 MiB (GPU 0; 10.75 GiB total capacity; 320.87 MiB already allocated; 156.94 MiB free; 356.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "vae = vae.to(torch_device)\n",
    "text_encoder = text_encoder.to(torch_device)\n",
    "unet = unet.to(torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be838342",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(latents,text_embeddings):\n",
    "    \n",
    "    num_inference_steps = 50            # Number of denoising steps\n",
    "\n",
    "    guidance_scale = 7.5                # Scale for classifier-free guidance\n",
    "\n",
    "    #############################################\n",
    "\n",
    "    uncond_input = tokenizer(\n",
    "        [\"\"] * batch_size, padding=\"max_length\", return_tensors=\"pt\"\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "      uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0]\n",
    "\n",
    "    ###############################################\n",
    "\n",
    "    embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
    "\n",
    "    #############################################\n",
    "\n",
    "    scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "    ##########################################\n",
    "\n",
    "    latents_1 = latents * scheduler.init_noise_sigma\n",
    "\n",
    "    #############################################\n",
    "\n",
    "\n",
    "\n",
    "    for t in tqdm(scheduler.timesteps):\n",
    "      # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
    "      latent_model_input_a = torch.cat([latents_1] * 2)\n",
    "\n",
    "      latent_model_input = scheduler.scale_model_input(latent_model_input_a, t)\n",
    "\n",
    "      # predict the noise residual\n",
    "      with torch.no_grad():\n",
    "        noise_pred = unet(latent_model_input, t, encoder_hidden_states=embeddings).sample\n",
    "\n",
    "      # perform guidance\n",
    "      noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "      noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "      # compute the previous noisy sample x_t -> x_t-1\n",
    "      latents_1 = scheduler.step(noise_pred, t, latents_1).prev_sample\n",
    "\n",
    "    ###############################################\n",
    "    # scale and decode the image latents with vae\n",
    "    latents_3 = 1 / 0.18215 * latents_1\n",
    "\n",
    "    with torch.no_grad():\n",
    "      image = vae.decode(latents_3).sample\n",
    "\n",
    "\n",
    "    ##########################################\n",
    "\n",
    "    image_1 = (image / 2 + 0.5).clamp(0, 1)\n",
    "    image_2 = image_1.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "    images = (image_2 * 255).round().astype(\"uint8\")\n",
    "    pil_images = [Image.fromarray(image) for image in images]\n",
    "    result_image = pil_images[0]\n",
    "    \n",
    "    return result_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b04764ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gaの関数\n",
    "population = 9 #個体数\n",
    "X = 3\n",
    "Y = 3\n",
    "mutation = 0 #突然変異の個体数\n",
    "# population = 3 #個体数\n",
    "# X = 1\n",
    "# Y = 3\n",
    "mutation = 2 #突然変異の個体数\n",
    "\n",
    "if X*Y!=population or population>9:\n",
    "    print('Error: parameter error', file=sys.stderr)\n",
    "    sys.exit(1)\n",
    "    \n",
    "gene_length = 64\n",
    "elite = 0 #エリートの数\n",
    "initializa_txt_num = 70 #初期化個体においてtxtのベクトルをどれくらい元から変異させるか\n",
    "batch_size = 1\n",
    "height = 512                        # default height of Stable Diffusion\n",
    "width = 512                        # default width of Stable Diffusion\n",
    "image_mutation_rate = 400\n",
    "text_mutation_rate = 500\n",
    "\n",
    "if elite+mutation>population:\n",
    "    print('Error: parameter error', file=sys.stderr)\n",
    "    sys.exit(1)\n",
    "\n",
    "#seed値を複数個用意\n",
    "def initialize_gene(text_embeddings):\n",
    "    arr = []\n",
    "    for i in range(population):\n",
    "        embeddings = text_embeddings.clone()\n",
    "        seed_here = random.randrange(1000)\n",
    "        latents_torch = torch.randn(\n",
    "          (batch_size, unet.in_channels, height // 8, width // 8),\n",
    "          generator=torch.manual_seed(seed_here),\n",
    "        )\n",
    "        latents = latents_torch.to(torch_device)\n",
    "        for j in range(initializa_txt_num):\n",
    "            a = random.randrange(77)\n",
    "            b = random.randrange(768)\n",
    "            text_embeddings[0][a][b] = np.random.randn()\n",
    "        arr.append([latents,embeddings])    \n",
    "    return arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e55ee11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image(diffusion_images,epoch,folder):\n",
    "    os.mkdir(folder+'/epoch'+str(epoch))\n",
    "    for i in range (len(diffusion_images)):\n",
    "        diffusion_images[i].save(folder+\"/epoch\"+str(epoch)+\"/\"+str(i)+\".png\")\n",
    "    \n",
    "    \n",
    "def evolve(selected,genes):\n",
    "    global evolve_explanation\n",
    "    evolve_explanation = \"\"\n",
    "    new_genes = []\n",
    "    \n",
    "    #エリート戦略 #選ばれた画像は次世代に残す(?)\n",
    "#     for i in range(population):\n",
    "#         if selected[i]==1:\n",
    "#             adding_gene = copy.deepcopy(genes[i])\n",
    "#             evolve_explanation += str(len(new_genes))+\". elite\"+str(i)+\"\\n\"\n",
    "#             new_genes.append(adding_gene)\n",
    "    \n",
    "    #mutation, cross overで使う配列の準備\n",
    "    index = []\n",
    "    num = 0\n",
    "    for i in range(population):\n",
    "        num += 1+selected[i]*adapt\n",
    "        index.append(num)\n",
    "    index.append(num)\n",
    "    \n",
    "    #mutation\n",
    "    for i in range(mutation):\n",
    "        p1 = random.randrange(num)\n",
    "        g1 = bisect.bisect(index,p1)\n",
    "        evolve_explanation += str(len(new_genes))+\". mutation \"+str(g1)+\"\\n\"\n",
    "        new_gene1 = copy.deepcopy(genes[g1])\n",
    "        # 画像の元\n",
    "        for j in range(4):\n",
    "            for k in range(64):\n",
    "                for l in range(64):\n",
    "                    mutation_flag = random.randrange(image_mutation_rate)\n",
    "                    if mutation_flag==0:\n",
    "                        new_gene1[0][0][j][k][l] = np.random.randn()\n",
    "        #textの元\n",
    "        for j in range(77):\n",
    "            for k in range(768):\n",
    "                mutation_flag = random.randrange(text_mutation_rate)\n",
    "                if mutation_flag==0:# cross over\n",
    "                    new_gene1[1][0][j][k] = np.random.randn()\n",
    "        new_genes.append(new_gene1)\n",
    "    # cross over\n",
    "    for i in range(population-len(new_genes)):\n",
    "        p1 = random.randrange(num)\n",
    "        p2 = random.randrange(num)\n",
    "        g1 = bisect.bisect(index,p1)\n",
    "        g2 = bisect.bisect(index,p2)\n",
    "        new_gene = copy.deepcopy(genes[g1])\n",
    "        new_gene1 = copy.deepcopy(genes[g2])\n",
    "        evolve_explanation += str(len(new_genes))+\". crossover \"+str(g1)+\" and \"+str(g2)+\"\\n\"\n",
    "        cross = random.randrange(gene_length)\n",
    "        for j in range(4):\n",
    "            for k in range(64):\n",
    "                for l in range(64):\n",
    "                    a = random.randrange(2)\n",
    "                    if a==0:\n",
    "                        new_gene[0][0][j][k][l] = new_gene1[0][0][j][k][l]\n",
    "        for j in range(77):\n",
    "            for k in range(768):\n",
    "                new_gene[1][0][j][k] = new_gene1[1][0][j][k]\n",
    "        new_genes.append(new_gene)\n",
    "                    \n",
    "    if len(new_genes)!=population:\n",
    "        print('Error: evolve error', file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "    return new_genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca641d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kobayashi/anaconda3/envs/pytorch/lib/python3.9/tkinter/__init__.py\", line 1892, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"/tmp/ipykernel_781701/3533418703.py\", line 595, in switch_to_live\n",
      "    display_first_prompt_input_screen()\n",
      "  File \"/tmp/ipykernel_781701/3533418703.py\", line 526, in display_first_prompt_input_screen\n",
      "    img = Image.open(sample_image_path)\n",
      "  File \"/home/kobayashi/anaconda3/envs/pytorch/lib/python3.9/site-packages/PIL/Image.py\", line 3092, in open\n",
      "    fp = builtins.open(filename, \"rb\")\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'sea_image.png'\n",
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kobayashi/anaconda3/envs/pytorch/lib/python3.9/tkinter/__init__.py\", line 1892, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"/tmp/ipykernel_781701/3533418703.py\", line 409, in display_iec_choosing_screen\n",
      "    text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\n",
      "  File \"/home/kobayashi/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/kobayashi/anaconda3/envs/pytorch/lib/python3.9/site-packages/transformers/models/clip/modeling_clip.py\", line 721, in forward\n",
      "    return self.text_model(\n",
      "  File \"/home/kobayashi/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/kobayashi/anaconda3/envs/pytorch/lib/python3.9/site-packages/transformers/models/clip/modeling_clip.py\", line 631, in forward\n",
      "    hidden_states = self.embeddings(input_ids=input_ids, position_ids=position_ids)\n",
      "  File \"/home/kobayashi/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/kobayashi/anaconda3/envs/pytorch/lib/python3.9/site-packages/transformers/models/clip/modeling_clip.py\", line 165, in forward\n",
      "    inputs_embeds = self.token_embedding(input_ids)\n",
      "  File \"/home/kobayashi/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/kobayashi/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/sparse.py\", line 160, in forward\n",
      "    return F.embedding(\n",
      "  File \"/home/kobayashi/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/functional.py\", line 2210, in embedding\n",
      "    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n",
      "RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper__index_select)\n"
     ]
    }
   ],
   "source": [
    "##########################################################\n",
    "# tkinterの参考にしたサイト\n",
    "# https://teratail.com/questions/202187\n",
    "# buttonに関するサイト\n",
    "# https://torimakujoukyou.com/python-tkinter-for-button/\n",
    "# https://qiita.com/igor-bond16/items/39c8b75d844f30cea197\n",
    "# 画像の切り替え\n",
    "# https://joytas.net/programming/python/tkinter-img\n",
    "# 画面の切り替え\n",
    "# https://office54.net/python/tkinter/screen-change-tkraise\n",
    "##########################################################\n",
    "\n",
    "#ライブラリのインポート\n",
    "import tkinter as tk\n",
    "from tkinter import ttk\n",
    "from tkinter import messagebox\n",
    "import random\n",
    "import PIL\n",
    "from PIL import Image,ImageTk\n",
    "##############################################\n",
    "#parameter\n",
    "image_padding = 300\n",
    "imgs = [] # Imageを保持するリストを追加\n",
    "iec_buttons = []\n",
    "iec_selected = []\n",
    "iec_selected_image_id = -1\n",
    "conv_selected_image_id = -1\n",
    "canvases = []\n",
    "conv_canvases = []\n",
    "labels = []\n",
    "for i in range(X*Y):\n",
    "    iec_selected.append(0)\n",
    "iec_epoch = 0 #iecでの試行回数\n",
    "conv_epoch = 0 #従来手法での試行回数\n",
    "adapt = 5 #iecにおいてuserに選ばれた遺伝子は選ばれていない遺伝子よりadapt倍選ばれやすい\n",
    "evolve_explanation = \"evolution explanation\"\n",
    "#diffusionの処理\n",
    "iec_target_theme = \"シンデレラ\"\n",
    "sample_image_path = \"sea_image.png\" #iecシステムの使い方を説明するときのサンプル画像\n",
    "sample_image_prompt = \"an illustration,　in the fantastic tropical sea, many fish, dazzling light coming from above, rocky shore\"\n",
    "sample_image_folder_path = \"sample_image\"\n",
    "conv_target_theme = \"かぐや姫\"\n",
    "prompt = []\n",
    "iec_experiment_done = False #iecシステムの実験が終わったか\n",
    "conv_experiment_done = False #stable diffusionシステムの実験が終わったか\n",
    "current_system_is_practice1 = False\n",
    "current_system_is_practice2 = False\n",
    "current_system_is_live = False\n",
    "practice_generation_limit = 3#練習時の試行回数の上限\n",
    "live_generation_limit = 10#本番時の試行回数の上限\n",
    "##############################################\n",
    "def set_target_title(type,screen):\n",
    "    if type==\"iec\":\n",
    "        target_label = tk.Label(screen,text=\"theme\", fg=\"white\", foreground='blue',font=(\"Lucida Console\",\"15\"))\n",
    "        target_label.place(x=150, y=70)\n",
    "        target_theme_label = tk.Label(screen,text=iec_target_theme, foreground='black',font=(\"Lucida Console\",\"15\"))\n",
    "        target_theme_label.place(x=150, y=100)\n",
    "    elif type==\"conv\":\n",
    "        target_label = tk.Label(screen,text=\"theme\", fg=\"white\", foreground='blue',font=(\"Lucida Console\",\"15\"))\n",
    "        target_label.place(x=150, y=70)\n",
    "        target_theme_label = tk.Label(screen,text=conv_target_theme, foreground='black',font=(\"Lucida Console\",\"15\"))\n",
    "        target_theme_label.place(x=150, y=100)\n",
    "    else:\n",
    "        print('Error: target image invalid input', file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "def showing_progress_screen_and_generate_images(type,screen):\n",
    "    # ログ\n",
    "    with open('./user_experiment_data/'+experiment_id+'/log.txt', 'a') as f:\n",
    "        print(time.time(), \"progress_screen_is_displayed.\", file=f)\n",
    "    #プログレス画面\n",
    "    progress_screen = tk.Frame(screen,bg='#505050',width=1500,height=900)\n",
    "    progress_screen.lift()\n",
    "    progress_screen.pack(fill=tk.BOTH)\n",
    "    #プログレスバー\n",
    "    #プログレスバーの初期設定\n",
    "    pbval = tk.IntVar(value=0)\n",
    "    progressbar=ttk.Progressbar(progress_screen,orient=\"horizontal\",length=700,mode=\"determinate\")\n",
    "    progressbar.place(x=400,y=400)\n",
    "    progressbar.configure(maximum=X*Y,variable=pbval)\n",
    "    #label\n",
    "    progress_label1 = tk.Label(progress_screen,text=\"generating images\", background='#505050',  foreground='#ccffff',font=(\"Lucida Console\",\"15\"))\n",
    "    progress_label1.place(x=400, y=375)\n",
    "    progress_label2 = tk.Label(progress_screen,text=\"please wait...\", background='#505050', foreground='red',font=(\"Lucida Console\",\"10\"))\n",
    "    progress_label2.place(x=1000, y=425)\n",
    "    root.update()\n",
    "    ##############################################\n",
    "    #画像を生成する\n",
    "    print(type+\": images are generating\")\n",
    "    diffusion_images = []\n",
    "    if type==\"iec\":\n",
    "        for i in range(population):\n",
    "            diffusion_image = func(genes[i][0],genes[i][1])\n",
    "            diffusion_images.append(diffusion_image)\n",
    "            pbval.set(pbval.get() + 1)\n",
    "            root.update()\n",
    "        save_image(diffusion_images,iec_epoch,iec_image_save_path)\n",
    "    elif type==\"conv\":\n",
    "        global prompt\n",
    "        global conv_epoch\n",
    "        prompt = []\n",
    "        input_prompt = user_input_prompt.get(1.0, tk.END+\"-1c\")\n",
    "        # ログ\n",
    "        with open('./user_experiment_data/'+experiment_id+'/log.txt', 'a') as f:\n",
    "            print(time.time(), \"conv_epoch\", conv_epoch, \"input_prompt\",input_prompt, file=f)\n",
    "        text_input = tokenizer([input_prompt], padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\n",
    "        diffusion_images = []\n",
    "        for i in range(population):\n",
    "            seed_here = random.randrange(1000)\n",
    "            latents_torch = torch.randn(\n",
    "              (batch_size, unet.in_channels, height // 8, width // 8),\n",
    "              generator=torch.manual_seed(seed_here),\n",
    "            )\n",
    "            latents = latents_torch.to(torch_device)\n",
    "            diffusion_image = func(latents,text_embeddings)\n",
    "            diffusion_images.append(diffusion_image)\n",
    "            pbval.set(pbval.get() + 1)\n",
    "            root.update()\n",
    "        save_image(diffusion_images,conv_epoch,stable_diffusion_image_save_path)\n",
    "    elif type==\"px1\" or type==\"px2\":\n",
    "        for i in range(population):\n",
    "            time.sleep(1.0/population)\n",
    "            pbval.set(pbval.get() + 1)\n",
    "            root.update()\n",
    "    else:\n",
    "        print('Error: progress screen invalid input', file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "    progress_screen.destroy() #prompt入力画面を削除する\n",
    "# web表示用の関数\n",
    "# 選択されたときにボタンの色を変えて、選ばれた画像を記録\n",
    "def click_button(item,btn):\n",
    "    def nothing():\n",
    "        if iec_selected[item]==1:\n",
    "            btn.config(bg='#E6E6E6')\n",
    "            iec_selected[item] = 0\n",
    "        else:\n",
    "            btn.config(bg='yellow')\n",
    "            iec_selected[item] = 1\n",
    "    return nothing\n",
    "# evolveボタンが押されたときに進化させて新しい画像を表示\n",
    "def click_evolve_button():\n",
    "    global iec_epoch\n",
    "    #################\n",
    "    # stable diffusionのコード\n",
    "    #進化+選択\n",
    "    # ログ\n",
    "    with open('./user_experiment_data/'+experiment_id+'/log.txt', 'a') as f:\n",
    "        print(time.time(), \"iec_epoch\", iec_epoch, \"selected population\", iec_selected, file=f)\n",
    "    #進化\n",
    "    global genes\n",
    "    genes = evolve(iec_selected,genes)\n",
    "    #画像生成\n",
    "    iec_epoch += 1\n",
    "    if current_system_is_live:\n",
    "        showing_progress_screen_and_generate_images(\"iec\",evolve_screen)\n",
    "    elif current_system_is_practice1:\n",
    "        showing_progress_screen_and_generate_images(\"px1\",evolve_screen)\n",
    "    else:\n",
    "        print('Error: evolve system error', file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "    ###################\n",
    "    # ログ\n",
    "    with open('./user_experiment_data/'+experiment_id+'/log.txt', 'a') as f:\n",
    "        print(time.time(), \"iec_epoch\", iec_epoch, \"new images are displayed\", file=f)\n",
    "    #新しい画像を表示\n",
    "    iec_image_path = \"sample_image\"\n",
    "    for i in range(X*Y):\n",
    "        if current_system_is_live:\n",
    "            iec_image_path = iec_image_save_path+'/epoch'+str(iec_epoch)+'/' + str(i) + '.png'\n",
    "        elif current_system_is_practice1:\n",
    "            iec_image_path = sample_image_folder_path+'/epoch'+str(iec_epoch)+'/' +str(i) + '.png'\n",
    "        img = Image.open(iec_image_path)\n",
    "        img = img.resize((256,256),Image.ANTIALIAS)\n",
    "        img = ImageTk.PhotoImage(img)\n",
    "        imgs.append(img) # Imageをリストに追加\n",
    "        canvases[i].delete()\n",
    "        canvases[i].create_image(3, 3, image=img, anchor=tk.NW,tag=str(i))\n",
    "        #ボタンを未クリックにする\n",
    "        iec_buttons[i].config(bg='#E6E6E6')\n",
    "        iec_selected[i] = 0\n",
    "    labels[0].config(text=evolve_explanation)\n",
    "    # ログ\n",
    "    with open('./user_experiment_data/'+experiment_id+'/log.txt', 'a') as f:\n",
    "        print(time.time(), \"iec_epoch\", iec_epoch, \"evolve_explanation\",evolve_explanation, file=f)\n",
    "#最後の結果画面\n",
    "def display_final_result_screen():\n",
    "    experiment_end_screen.destroy()\n",
    "    # ログ\n",
    "    with open('./user_experiment_data/'+experiment_id+'/log.txt', 'a') as f:\n",
    "        print(time.time(), \"final_result_screen_is_displayed.\", file=f)\n",
    "    # iec target画像の設置\n",
    "    set_target_title(\"iec\",root)\n",
    "    # iec generated画像の設置\n",
    "    img = Image.open(iec_image_save_path+'/epoch'+str(iec_epoch)+'/' + str(iec_selected_image_id) + '.png')\n",
    "    img = img.resize((256,256),Image.ANTIALIAS)\n",
    "    img = ImageTk.PhotoImage(img)\n",
    "    imgs.append(img) # Imageをリストに追加\n",
    "    canvas = tk.Canvas(root, width=256, height=256)\n",
    "    canvas.place(x=500, y=100) \n",
    "    canvas.create_image(3, 3, image=img, anchor=tk.NW)\n",
    "    iec_generated_label = tk.Label(root,text=\"image 1\", foreground='blue',font=(\"Lucida Console\",\"15\"))\n",
    "    iec_generated_label.place(x=500, y=70)\n",
    "    # stable diffusion target labelの設置\n",
    "    target_label = tk.Label(root,text=\"theme\", foreground='blue',font=(\"Lucida Console\",\"15\"))\n",
    "    target_label.place(x=150, y=470)\n",
    "    target_theme_label = tk.Label(root,text=conv_target_theme, foreground='black',font=(\"Lucida Console\",\"15\"))\n",
    "    target_theme_label.place(x=150, y=500)\n",
    "    # stable diffusion generated画像の設置\n",
    "    img = Image.open(stable_diffusion_image_save_path+'/epoch'+str(conv_epoch)+'/' + str(conv_selected_image_id) + '.png')\n",
    "    img = img.resize((256,256),Image.ANTIALIAS)\n",
    "    img = ImageTk.PhotoImage(img)\n",
    "    imgs.append(img) # Imageをリストに追加\n",
    "    canvas = tk.Canvas(root,width=256, height=256)\n",
    "    canvas.place(x=500, y=500) \n",
    "    canvas.create_image(3, 3, image=img, anchor=tk.NW)\n",
    "    iec_generated_label = tk.Label(root,text=\"image 2\", foreground='blue',font=(\"Lucida Console\",\"15\"))\n",
    "    iec_generated_label.place(x=500, y=470)\n",
    "#実験終了画面\n",
    "def display_experiment_end_screen():\n",
    "    if reverse_experiment_order.get()==False:\n",
    "        #iec->conv\n",
    "        global conv_selected_image_id\n",
    "        #画像を未選択だったときにエラー処理\n",
    "        conv_selected_image_id = conv_now_selected_image_id.get()\n",
    "        if conv_selected_image_id==population:\n",
    "            messagebox.showerror('Error', 'Please select one image before you press the end button')\n",
    "            return\n",
    "        # 実験済みにチェックをつける\n",
    "        global conv_experiment_done\n",
    "        conv_experiment_done=True\n",
    "        #進化計算画面を削除する\n",
    "        conv_choosing_screen.destroy()\n",
    "    else:\n",
    "        #conv->iec\n",
    "        global iec_selected_image_id\n",
    "        #画像を未選択だったときにエラー処理\n",
    "        iec_selected_image_id = iec_now_selected_image_id.get()\n",
    "        if iec_selected_image_id==population:\n",
    "            messagebox.showerror('Error', 'Please select one image before you press the end button')\n",
    "            return\n",
    "        # 実験済みにチェックをつける\n",
    "        global iec_experiment_done\n",
    "        iec_experiment_done=True\n",
    "        #進化計算画面を削除する\n",
    "        evolve_screen.destroy()\n",
    "    # ログ\n",
    "    with open('./user_experiment_data/'+experiment_id+'/log.txt', 'a') as f:\n",
    "        print(time.time(), \"experiment_end_screen_is_displayed\", file=f)\n",
    "    messagebox.showinfo('good job!', 'This is the end of this program')\n",
    "    #実験終了画面でのframe作成\n",
    "    global experiment_end_screen\n",
    "    experiment_end_screen = tk.Frame(root,width=1500,height=900)\n",
    "    experiment_end_screen.pack(fill=tk.BOTH)\n",
    "    #スコア表示ボタンボタン\n",
    "    display_result_button = tk.Button(experiment_end_screen,text = \"result\", fg=\"white\", bg=\"blue\",command=display_final_result_screen)\n",
    "    display_result_button.place(x=690,y=440)\n",
    "#stable diffusionでの画像生成\n",
    "def click_generate_button():\n",
    "    global conv_epoch\n",
    "    conv_epoch += 1\n",
    "    #ui上に画像を表示する\n",
    "    #stable diffusionで画像を生成する\n",
    "    if current_system_is_live:\n",
    "        showing_progress_screen_and_generate_images(\"conv\",conv_choosing_screen)\n",
    "    elif current_system_is_practice2:\n",
    "        showing_progress_screen_and_generate_images(\"px2\",conv_choosing_screen)\n",
    "    else:\n",
    "        print('Error: system changing error', file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "    #第2世代以降の画像を表示する\n",
    "    conv_image_path = \"sample image\"\n",
    "    for i in range(X*Y):\n",
    "        if current_system_is_live:\n",
    "            conv_image_path = stable_diffusion_image_save_path+'/epoch'+str(conv_epoch)+'/' + str(i) + '.png'\n",
    "        elif current_system_is_practice2:\n",
    "            conv_image_path = sample_image_folder_path + '/epoch'+str(conv_epoch)+'/' + str(i) + '.png'\n",
    "        img = Image.open(conv_image_path)\n",
    "        img = img.resize((256,256),Image.ANTIALIAS)\n",
    "        img = ImageTk.PhotoImage(img)\n",
    "        imgs.append(img) # Imageをリストに追加\n",
    "        conv_canvases[i].delete()\n",
    "        conv_canvases[i].create_image(3, 3, image=img, anchor=tk.NW,tag=str(i))\n",
    "#従来手法で実験を行う\n",
    "def display_conv_choosing_screen():\n",
    "    global user_input_prompt\n",
    "    input_prompt=user_input_prompt.get(1.0, tk.END+\"-1c\")#frame_destroyの前に取得する必要がある\n",
    "    #stable diffusionで画像を生成する\n",
    "    if current_system_is_live:\n",
    "        showing_progress_screen_and_generate_images(\"conv\",first_prompt_input_screen)\n",
    "    elif current_system_is_practice2:\n",
    "        showing_progress_screen_and_generate_images(\"px2\",first_prompt_input_screen)\n",
    "    else:\n",
    "        print('Error: system changing error conv', file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "    #first prompt入力画面を削除する\n",
    "    first_prompt_input_screen.destroy()\n",
    "    # ログ\n",
    "    with open('./user_experiment_data/'+experiment_id+'/log.txt', 'a') as f:\n",
    "        print(time.time(), \"conv_choosing_screen_is_displayed.\", file=f)\n",
    "    #従来手法画像生成画面でのframe作成\n",
    "    global conv_choosing_screen\n",
    "    conv_choosing_screen = tk.Frame(root,width=1500,height=900)\n",
    "    conv_choosing_screen.pack(fill=tk.BOTH)\n",
    "    #初期世代の画像を表示する\n",
    "    conv_image_path = \"iec_sample_image\"\n",
    "    for i in range(X):\n",
    "        for j in range(Y):\n",
    "            # 選択肢の画像の設置\n",
    "            if current_system_is_live:\n",
    "                conv_image_path = stable_diffusion_image_save_path+'/epoch0/' + str(i*Y+j) + '.png'\n",
    "            elif current_system_is_practice2:\n",
    "                conv_image_path = sample_image_folder_path + '/epoch0/' + str(i*Y+j) + '.png'\n",
    "            img = Image.open(conv_image_path)\n",
    "            img = img.resize((256,256),Image.ANTIALIAS)\n",
    "            img = ImageTk.PhotoImage(img)\n",
    "            imgs.append(img)\n",
    "            canvas = tk.Canvas(conv_choosing_screen, width=256, height=256)\n",
    "            canvas.place(x=(j+2)*image_padding, y=i*image_padding) \n",
    "            canvas.create_image(3, 3, image=img, anchor=tk.NW,tag=str(i*Y+j))\n",
    "            conv_canvases.append(canvas)\n",
    "            # 選んでもらうボタンの設置\n",
    "            image_id_label = tk.Label(conv_choosing_screen, text=str(i*Y+j), width=5, height=2,bg=\"white\")\n",
    "            image_id_label.place(x=(j+2.35)*image_padding,y=(i+0.85)*image_padding)\n",
    "    #target画像の表示\n",
    "    set_target_title(\"conv\",conv_choosing_screen)\n",
    "    #prompt入力ボックスの設置\n",
    "    conv_prompt_frame = tk.Frame(conv_choosing_screen, bg=\"#C6C6C6\",width=256,height=300)\n",
    "    conv_prompt_frame.place(x=150,y=500)\n",
    "    # promptの入力画面を作成\n",
    "    conv_screen_explanation_frame = ttk.Label(conv_prompt_frame, text=\"input the description of \\nthe image\")\n",
    "    conv_screen_explanation_frame.pack(padx=5, pady=10, fill=tk.X)\n",
    "    user_input_prompt = tk.Text(conv_prompt_frame, height=10)\n",
    "    user_input_prompt.pack(padx=5, pady=10, fill=tk.X)\n",
    "    user_input_prompt.insert(1.0, input_prompt)\n",
    "    # generateボタン\n",
    "    conv_screen_generate_image_button = tk.Button(conv_prompt_frame,fg=\"white\", bg=\"blue\", text=\"generate\", command=click_generate_button)\n",
    "    conv_screen_generate_image_button.place(x=80,y=170)\n",
    "    # 気に入った唯一の画像を選ぶボタン\n",
    "    number_select = list(range(population+1))\n",
    "    global conv_now_selected_image_id\n",
    "    conv_now_selected_image_id = tk.IntVar()\n",
    "    number_select_pulldown = ttk.Combobox(\n",
    "        conv_prompt_frame, textvariable=conv_now_selected_image_id, state=\"readonly\",\n",
    "        values=number_select, width=10)\n",
    "    number_select_pulldown.set(population)\n",
    "    number_select_pulldown.place(x=40, y=275)\n",
    "    # end ボタン\n",
    "    conv_screen_fix_image_button = tk.Button(conv_prompt_frame,bg=\"red\", text=\"end\")\n",
    "    conv_screen_fix_image_button.place(x=150,y=275)\n",
    "    if current_system_is_practice2:\n",
    "        conv_screen_fix_image_button.config(command=display_start_screen)\n",
    "    elif reverse_experiment_order.get()==False:\n",
    "        #iec->conv\n",
    "        conv_screen_fix_image_button.config(command=display_experiment_end_screen)\n",
    "    else:\n",
    "        #conv->iec\n",
    "        conv_screen_fix_image_button.config(command=display_going_next_screen)\n",
    "    conv_choosing_screen.propagate(False)\n",
    "    conv_prompt_frame.propagate(False)\n",
    "##############################################\n",
    "# １つ目の実験が終わったときに次の実験に進むボタンの表示\n",
    "def display_going_next_screen():\n",
    "    if reverse_experiment_order.get()==False:\n",
    "        #iec->conv\n",
    "        global iec_selected_image_id\n",
    "        #画像を未選択だったときにエラー処理\n",
    "        iec_selected_image_id = iec_now_selected_image_id.get()\n",
    "        if iec_selected_image_id==population:\n",
    "            messagebox.showerror('Error', 'Please select one image before you press the end button')\n",
    "            return\n",
    "        # 実験済みにチェックをつける\n",
    "        global iec_experiment_done\n",
    "        iec_experiment_done=True\n",
    "        #進化計算画面を削除する\n",
    "        evolve_screen.destroy()\n",
    "    else:\n",
    "        #conv->iec\n",
    "        global conv_selected_image_id\n",
    "        #画像を未選択だったときにエラー処理\n",
    "        conv_selected_image_id = conv_now_selected_image_id.get()\n",
    "        if conv_selected_image_id==population:\n",
    "            messagebox.showerror('Error', 'Please select one image before you press the end button')\n",
    "            return\n",
    "        # 実験済みにチェックをつける\n",
    "        global conv_experiment_done\n",
    "        conv_experiment_done=True\n",
    "        #進化計算画面を削除する\n",
    "        conv_choosing_screen.destroy()\n",
    "    # ログ\n",
    "    with open('./user_experiment_data/'+experiment_id+'/log.txt', 'a') as f:\n",
    "        print(time.time(), \"going_next_screen_is_displayed\", file=f)\n",
    "    #次の実験に進む画面でのframe作成\n",
    "    global going_next_screen\n",
    "    going_next_screen = tk.Frame(root,width=1500,height=900)\n",
    "    going_next_screen.pack(fill=tk.BOTH)\n",
    "    #次の実験に進むボタン\n",
    "    going_next_button = tk.Button(going_next_screen,text = \"next experiment\", fg=\"white\", bg=\"blue\",command=display_first_prompt_input_screen)\n",
    "    going_next_button.place(x=690,y=440)\n",
    "# iec画面の作成\n",
    "def display_iec_choosing_screen():\n",
    "    input_prompt = user_input_prompt.get(1.0, tk.END+\"-1c\")#frame_destroyの前に取得する必要がある\n",
    "    # ログ\n",
    "    with open('./user_experiment_data/'+experiment_id+'/log.txt', 'a') as f:\n",
    "        print(time.time(), \"iec_input_prompt\", input_prompt, file=f)\n",
    "    # stable diffusionのコード\n",
    "    text_input = tokenizer([input_prompt], padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "      text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\n",
    "    global genes\n",
    "    genes = initialize_gene(text_embeddings)\n",
    "    first_prompt_input_screen.destroy() #prompt入力画面を削除する\n",
    "    if current_system_is_live:\n",
    "        showing_progress_screen_and_generate_images(\"iec\",root)\n",
    "    elif current_system_is_practice1:\n",
    "        showing_progress_screen_and_generate_images(\"px1\",root)\n",
    "    else:\n",
    "        print('Error: system changing error', file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "    # ログ\n",
    "    with open('./user_experiment_data/'+experiment_id+'/log.txt', 'a') as f:\n",
    "        print(time.time(), \"display_iec_choosing_screen_is_displayed.\", file=f)\n",
    "    #進化計算画面でのフレームの作成\n",
    "    global evolve_screen\n",
    "    evolve_screen = tk.Frame(root,width=1500,height=900)\n",
    "    evolve_screen.pack(fill=tk.BOTH)\n",
    "    #初期世代の画像を表示する\n",
    "    iec_image_path = \"iec_sample_image\"\n",
    "    for i in range(X):\n",
    "        for j in range(Y):\n",
    "            # 選択肢の画像の設置\n",
    "            if current_system_is_live:\n",
    "                iec_image_path = iec_image_save_path+'/epoch0/' + str(i*Y+j) + '.png'\n",
    "            elif current_system_is_practice1:\n",
    "                iec_image_path = sample_image_folder_path + '/epoch0/' + str(i*Y+j) + '.png'\n",
    "            img = Image.open(iec_image_path)\n",
    "            img = img.resize((256,256),Image.ANTIALIAS)\n",
    "            img = ImageTk.PhotoImage(img)\n",
    "            imgs.append(img)\n",
    "            canvas = tk.Canvas(evolve_screen, width=256, height=256)\n",
    "            canvas.place(x=(j+2)*image_padding, y=i*image_padding) \n",
    "            canvas.create_image(3, 3, image=img, anchor=tk.NW,tag=str(i*Y+j))\n",
    "            canvases.append(canvas)\n",
    "            # 選んでもらうボタンの設置\n",
    "            button = tk.Button(evolve_screen, text=str(i*Y+j),width=3, height=1, bg=\"white\")\n",
    "            button.place(x=(j+2.35)*image_padding,y=(i+0.85)*image_padding)\n",
    "            button.config(command=click_button(i*Y+j,button))\n",
    "            iec_buttons.append(button)\n",
    "    evolve_explanation = input_prompt #説明文\n",
    "    evolution_label = tk.Message(evolve_screen,text=evolve_explanation,  foreground='black',font=(\"Lucida Console\",\"15\"), width=256)\n",
    "    evolution_label.place(x=150, y=400)\n",
    "    labels.append(evolution_label)\n",
    "    # ボタン用のフレーム\n",
    "    iec_navigate_frame = tk.Frame(evolve_screen, bg=\"#C6C6C6\",width=256,height=100)\n",
    "    iec_navigate_frame.place(x=150,y=500)\n",
    "    #進化させるボタン\n",
    "    evolve_button = tk.Button(iec_navigate_frame,text = \"continue\", fg=\"white\", bg=\"blue\",command=click_evolve_button)\n",
    "    evolve_button.place(x=85,y=10)\n",
    "    # 気に入った唯一の画像を選ぶボタン\n",
    "    global iec_now_selected_image_id\n",
    "    iec_now_selected_image_id = tk.IntVar()\n",
    "    iec_number_select_pulldown = ttk.Combobox(\n",
    "        iec_navigate_frame, textvariable=iec_now_selected_image_id, state=\"readonly\", \n",
    "        values=list(range(population+1)), width=10)\n",
    "    iec_number_select_pulldown.set(population)\n",
    "    iec_number_select_pulldown.place(x=40, y=75)\n",
    "    # 終了ボタン\n",
    "    end_button = tk.Button(iec_navigate_frame,text = \"end\", bg=\"red\")\n",
    "    end_button.place(x=150,y=75)\n",
    "    \n",
    "    if current_system_is_practice1:\n",
    "        end_button.config(command=display_start_screen)\n",
    "    elif reverse_experiment_order.get()==False:\n",
    "        #iec->conv\n",
    "        end_button.config(command=display_going_next_screen)\n",
    "    else:\n",
    "        #conv->iec\n",
    "        end_button.config(command=display_experiment_end_screen)\n",
    "    # target画像の設置\n",
    "    set_target_title(\"iec\",evolve_screen)\n",
    "#promptを初めて入力するときの入力画面の作成\n",
    "def display_first_prompt_input_screen():\n",
    "    if iec_experiment_done==False and conv_experiment_done==False:\n",
    "        start_screen.destroy()\n",
    "    elif iec_experiment_done==False or conv_experiment_done==False:\n",
    "        going_next_screen.destroy()\n",
    "    else:\n",
    "        print('Error: screen switching error', file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "    # ログ\n",
    "    with open('./user_experiment_data/'+experiment_id+'/log.txt', 'a') as f:\n",
    "        print(time.time(), \"first_prompt_input_screen_is_displayed.\", file=f)\n",
    "    global first_prompt_input_screen\n",
    "    first_prompt_input_screen = tk.Frame(root,width=1500,height=900)\n",
    "    first_prompt_input_screen.pack(fill=tk.BOTH)\n",
    "    prompt_frame = tk.Frame(first_prompt_input_screen,width=400,height=350)\n",
    "    prompt_frame.place(x=650,y=50)\n",
    "    # promptの入力画面を作成\n",
    "    first_prompt_input_screen_explanation_frame = tk.Label(prompt_frame, bg='#C6C6C6',text=\"input the description of the image\")\n",
    "    first_prompt_input_screen_explanation_frame.pack(padx=5, pady=10, fill=tk.X)\n",
    "    global user_input_prompt\n",
    "    user_input_prompt = tk.Text(prompt_frame, height=10,font=(\"Ariel\",\"15\"))\n",
    "    user_input_prompt.pack(padx=5, pady=20, fill=tk.X)\n",
    "    #generateボタン\n",
    "    first_prompt_input_screen_iec_generate_image_button = tk.Button(prompt_frame,fg=\"#FFFFFF\", \n",
    "                               width=5,height=6,bg='#1E88E5',text=\"決定\\ngenerate images\",font=('gothic',\"30\"))\n",
    "    first_prompt_input_screen_iec_generate_image_button.pack(padx=5, pady=10, fill=tk.X)\n",
    "    if current_system_is_practice1 or (current_system_is_live and iec_experiment_done==False and conv_experiment_done==False and reverse_experiment_order.get()==False)or (iec_experiment_done==False and conv_experiment_done and reverse_experiment_order.get()):\n",
    "        # generateを押すとiec画像生成\n",
    "        first_prompt_input_screen_iec_generate_image_button.config(command=display_iec_choosing_screen)\n",
    "        # お題の設置\n",
    "        set_target_title(\"iec\",first_prompt_input_screen)\n",
    "    elif current_system_is_practice2 or (iec_experiment_done and conv_experiment_done==False and reverse_experiment_order.get()==False) or (iec_experiment_done==False and conv_experiment_done==False and reverse_experiment_order.get()):\n",
    "        # generateを押すとconv画像生成\n",
    "        first_prompt_input_screen_iec_generate_image_button.config(command=display_conv_choosing_screen)\n",
    "        # お題の設置\n",
    "        set_target_title(\"conv\",first_prompt_input_screen)\n",
    "    else:\n",
    "        print('Error: screen switching error', file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "    first_prompt_input_screen.propagate(False)\n",
    "    prompt_frame.propagate(False)\n",
    "    # 写真のサンプルを提示\n",
    "    showing_sample_screen = tk.Frame(first_prompt_input_screen,bg='#C6C6C6',width=1500,height=450)\n",
    "    showing_sample_screen.place(x=0,y=450)\n",
    "    img = Image.open(sample_image_path)\n",
    "    img = img.resize((256,256),Image.ANTIALIAS)\n",
    "    img = ImageTk.PhotoImage(img)\n",
    "    imgs.append(img) # Imageをリストに追加\n",
    "    canvas = tk.Canvas(first_prompt_input_screen, width=256, height=256, bg='#C6C6C6')\n",
    "    canvas.place(x=150, y=500) \n",
    "    canvas.create_image(3, 3, image=img, anchor=tk.NW,tag=\"sample image\")\n",
    "    target_label = tk.Label(first_prompt_input_screen,text=\"sample image\", background='#C6C6C6',foreground='blue',font=(\"Lucida Console\",\"15\"))\n",
    "    target_label.place(x=150, y=470)\n",
    "    # promptのサンプルを提示\n",
    "    sample_prompt_frame = tk.Frame(first_prompt_input_screen,width=400,height=256)\n",
    "    sample_prompt_frame.place(x=650,y=500)\n",
    "    sample_prompt_explanation_label = ttk.Label(sample_prompt_frame, text=\"This is an example of the description\")\n",
    "    sample_prompt_explanation_label.pack(padx=5, pady=10, fill=tk.X)\n",
    "    sample_prompt_label = tk.Message(sample_prompt_frame, bg='#C6C6C6',text=sample_image_prompt,width=380)\n",
    "    sample_prompt_label.pack(padx=5, pady=10, fill=tk.X)\n",
    "    sample_prompt_frame.propagate(False)\n",
    "# 練習1に飛んだら練習1のグローバル変数をTrueにする\n",
    "def switch_to_practice1():\n",
    "    iec_image_save_path='./user_experiment_data/practice/iec'\n",
    "    stable_diffusion_image_save_path='./user_experiment_data/practice/stable_diffusion'\n",
    "    global experiment_id\n",
    "    experiment_id = \"practice\"\n",
    "    #ディレクトリ初期化\n",
    "    if os.path.exists('./user_experiment_data/practice'):\n",
    "        shutil.rmtree('./user_experiment_data/practice')\n",
    "    os.mkdir('./user_experiment_data/practice')\n",
    "    os.mkdir(iec_image_save_path)\n",
    "    os.mkdir(stable_diffusion_image_save_path)\n",
    "    global current_system_is_practice1,current_system_is_practice2,current_system_is_live\n",
    "    current_system_is_practice1 = True\n",
    "    current_system_is_practice2 = False\n",
    "    current_system_is_live = False\n",
    "    display_first_prompt_input_screen()\n",
    "# 練習2に飛んだら練習2のグローバル変数をTrueにする\n",
    "def switch_to_practice2():\n",
    "    iec_image_save_path='./user_experiment_data/practice/iec'\n",
    "    stable_diffusion_image_save_path='./user_experiment_data/practice/stable_diffusion'\n",
    "    global experiment_id\n",
    "    experiment_id = \"practice\"\n",
    "    #ディレクトリ初期化\n",
    "    if os.path.exists('./user_experiment_data/practice'):\n",
    "        shutil.rmtree('./user_experiment_data/practice')\n",
    "    os.mkdir('./user_experiment_data/practice')\n",
    "    os.mkdir(iec_image_save_path)\n",
    "    os.mkdir(stable_diffusion_image_save_path)\n",
    "    global current_system_is_practice1,current_system_is_practice2,current_system_is_live\n",
    "    current_system_is_practice1 = False\n",
    "    current_system_is_practice2 = True\n",
    "    current_system_is_live = False\n",
    "    display_first_prompt_input_screen()\n",
    "# 本番に飛んだら本番のグローバル変数をTrueにする\n",
    "def switch_to_live():\n",
    "    #被験者のidを振る\n",
    "    experiment_time_delta = datetime.timedelta(hours=9)\n",
    "    experiment_JST = datetime.timezone(experiment_time_delta, 'JST')\n",
    "    global experiment_id,iec_image_save_path,stable_diffusion_image_save_path\n",
    "    experiment_now = datetime.datetime.now(experiment_JST)\n",
    "    experiment_id = experiment_now.strftime('%Y%m%d%H%M%S')\n",
    "    iec_image_save_path='./user_experiment_data/'+experiment_id+'/iec'\n",
    "    stable_diffusion_image_save_path='./user_experiment_data/'+experiment_id+'/stable_diffusion'\n",
    "    #ディレクトリ初期化\n",
    "    os.mkdir('./user_experiment_data/'+experiment_id)\n",
    "    os.mkdir(iec_image_save_path)\n",
    "    os.mkdir(stable_diffusion_image_save_path)\n",
    "    global current_system_is_practice1,current_system_is_practice2,current_system_is_live\n",
    "    current_system_is_practice1 = False\n",
    "    current_system_is_practice2 = False\n",
    "    current_system_is_live =True\n",
    "    display_first_prompt_input_screen()\n",
    "def display_start_screen():\n",
    "    if current_system_is_practice1:\n",
    "        evolve_screen.destroy()\n",
    "    elif current_system_is_practice2:\n",
    "        conv_choosing_screen.destroy()\n",
    "    global start_screen\n",
    "    #webの初期配置コード\n",
    "    start_screen = tk.Frame(root,width=1500,height=900)\n",
    "    start_screen.pack(fill=tk.BOTH)\n",
    "    #練習1 ボタン\n",
    "    start_screen_practice1_button = tk.Button(start_screen,text=\"練習1\\npractice 1\", command=switch_to_practice1,font=(\"gothic\",\"50\"))\n",
    "    start_screen_practice1_button.config(bg=\"#1E88E5\", fg='#FFFFFF', width=20, height=3)\n",
    "    start_screen_practice1_button.place(x=450,y=100)\n",
    "    #練習2 ボタン\n",
    "    start_screen_practice2_button = tk.Button(start_screen,text=\"練習2\\npractice 2\", command=switch_to_practice2,font=(\"gothic\",\"50\"))\n",
    "    start_screen_practice2_button.config(bg=\"#42A5F5\", fg='#FFFFFF', width=20, height=3)\n",
    "    start_screen_practice2_button.place(x=450,y=350)\n",
    "    #本番 ボタン\n",
    "    start_screen_live_button = tk.Button(start_screen,text=\"本番\\nlive\", command=switch_to_live,font=(\"gothic\",\"50\"))\n",
    "    start_screen_live_button.config(bg=\"#E53935\", fg='#FFFFFF', width=20, height=3)\n",
    "    start_screen_live_button.place(x=450,y=600)\n",
    "    #各種パラメーター調整\n",
    "    #iecとstable diffusionの順序\n",
    "    global reverse_experiment_order\n",
    "    reverse_experiment_order = tk.BooleanVar()\n",
    "    experiment_order_checkbox = tk.Checkbutton(start_screen,variable=reverse_experiment_order, text='switch')\n",
    "    experiment_order_checkbox.place(x=50, y=850)\n",
    "if __name__ == \"__main__\":\n",
    "    with autocast(\"cuda\"):\n",
    "        ############################################################################\n",
    "        #webの初期配置(ptompt入力画面)\n",
    "        root = tk.Tk()\n",
    "        root.title(\"web browser\")\n",
    "        root.geometry(\"1500x900\")\n",
    "        display_start_screen()\n",
    "        root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1368d531",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
